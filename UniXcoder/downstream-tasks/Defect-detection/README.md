# CodeXGLUE -- Defect Detection
### The original CodexGlue ReadMe is modified by us.

## Task Definition

Given a source code, the task is to identify whether it is an insecure code that may attack software systems, such as resource leaks, use-after-free vulnerabilities and DoS attack.  We treat the task as binary classification (0/1), where 1 stands for insecure code and 0 for secure code.

### Dataset

The dataset we use comes from the paper [DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection
](https://arxiv.org/abs/2304.00409). We use the Devign dataset and the previous + Diverseful version splited 80%/10%/10% for training/dev/test. 
Note: the split is done randomly, and is made sure test data of Devign not included in the composed large dataset.


### Download and Preprocess

1.Download dataset from [website](https://huggingface.co/AISE-TUDelft/ML4SE23_G10_UnixCoder/tree/main) to "dataset" folder or do the following steps:

1. unzip the train, valid and test dataset of a specific type to dataset folder
2. Incase of Devign dataset, run changeDevign.py to fix dataset format
3. Rename to respective dataset to tran, valid, and test.jsonl

### Data Format

After preprocessing dataset, you can obtain three .jsonl files, i.e. train.jsonl, valid.jsonl, test.jsonl

For each file, each line in the uncompressed file represents one function.  One row is illustrated below.

   - **func:** the source code
   - **target:** 0 or 1 (vulnerability or not)
   - **idx:** the index of example

### Data Statistics

Data statistics of the dataset are shown in the below table:

| Dataset         | # Non-Vul Func | Vul Func |
| --------------- | :------------: | :------: |
| Devign          |     14,858     |  12,460  |
| ReVeal          |     20,494     |   2,240  |
| CVEFixes        |      9,134     |   6,515  |
| DiverseVul      |    311,547     |  18,945  |
| Composed Dataset|    356,033     |  40,160  |

## Evaluator

We provide a script to evaluate predictions for this task, and report accuracy score.

### Examples
```shell
python evaluator/evaluator.py -a evaluator/test.jsonl -p evaluator/predictions.txt
```

{'Acc': 0.6}

### Input predictions

A predications file that has predictions in TXT format, such as evaluator/predictions.txt. For example:

```shell
0	0
1	1
2	1
3	0
4	0
```

## Pipeline-CodeBERT

We also provide a pipeline that fine-tunes [UnixCoder-base-nine](https://huggingface.co/microsoft/unixcoder-base-nine) on this task.

### Fine-tune

```shell
cd code
python run.py \
    --output_dir=./saved_models \
    --model_type=roberta \
    --tokenizer_name=microsoft/unixcoder-base-nine \
    --model_name_or_path=microsoft/unixcoder-base-nine \
    --do_train \
    --train_data_file=../dataset/train.jsonl \
    --eval_data_file=../dataset/valid.jsonl \
    --test_data_file=../dataset/test.jsonl \
    --epoch 3 \
    --block_size 400 \
    --train_batch_size 32 \
    --eval_batch_size 64 \
    --learning_rate 2e-5 \
    --max_grad_norm 1.0 \
    --evaluate_during_training \
    --loss_func MSE\
    --BCE_bias 1.0\
    --seed 123456  2>&1 | tee train.log
```


### Inference

```shell
cd code
!python run.py \
    --output_dir=./saved_models \
    --model_type=roberta \
    --tokenizer_name=microsoft/unixcoder-base-nine \
    --model_name_or_path=microsoft/unixcoder-base-nine \
    --do_eval \
    --do_test \
    --train_data_file=../dataset/train.jsonl \
    --eval_data_file=../dataset/valid.jsonl \
    --test_data_file=../dataset/test.jsonl \
    --epoch 3 \
    --block_size 400 \
    --train_batch_size 32 \
    --eval_batch_size 64 \
    --learning_rate 2e-5 \
    --loss_func BCEWithLogits\
    --BCE_bias 10.0\
    --max_grad_norm 1.0 \
    --evaluate_during_training \
    --seed 123456 2>&1 | tee test.log

### Evaluation

```shell
python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt
```

{'Acc': 0.6207906295754027}

## Result

The results on the test set are shown as below:
| Methods               | Properties                                                                                                  |
| --------------------- | ----------------------------------------------------------------------------------------------------------- |
| Naive MSE loss function | Train Dataset: Devign, Test Dataset: Devign, Test samples: 2732, Epochs: 3, Loss multiplier: 1, Pos_weight: 1, Accuracy: 45.94%, F1 score: 0.606, Eval loss: 0.780 |
| BCE loss function       | Train Dataset: Devign, Test Dataset: Devign, Test samples: 2732, Epochs: 3, Loss multiplier: 1, Pos_weight: 1, Accuracy: 65.26%, F1 score: 0.605, Eval loss: 0.619 |
| BCE loss function       | Train Dataset: Composed, Test Dataset: Devign, Test samples: 2732, Epochs: 1, Loss multiplier: 1, Pos_weight: 10, Accuracy: 61.57%, F1 score: 0.561, Eval loss: 0.797 |
| BCE loss function       | Train Dataset: Composed, Test Dataset: Composed, Test samples: 37763, Epochs: 1, Loss multiplier: 1, Pos_weight: 1, Accuracy: 91.60%, F1 score: 0.273, Eval loss: 0.221 |
| BCE loss function       | Train Dataset: Composed, Test Dataset: Composed, Test samples: 37763, Epochs: 1, Loss multiplier: 10, Pos_weight: 1, Accuracy: 91.10%, F1 score: 0.365, Eval loss: 2.340 |
| BCE loss function       | Train Dataset: Composed, Test Dataset: Composed, Test samples: 37763, Epochs: 1, Loss multiplier: 1, Pos_weight: 10, Accuracy: 84.51%, F1 score: 0.398, Eval loss: 0.937 |
| BCE loss function       | Train Dataset: Improved Composed, Test Dataset: Devign, Test samples: 2732, Epochs: 0.2, Loss multiplier: 1, Pos_weight: 1, Accuracy: 91.89%, F1 score: 0.0033, Eval loss: 0.242 |
| BCE loss function       | Train Dataset: Improved Composed, Test Dataset: Devign, Test samples: 2732, Epochs: 0.2, Loss multiplier: 1, Pos_weight: 10, Accuracy: 91.89%, F1 score: 0.0033, Eval loss: 0.242 |


## Reference
<pre><code>@inproceedings{zhou2019devign,
  title={Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
  author={Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10197--10207},
  year={2019}
}
@inproceedings{chen2023diversevul,
  title={Diversevul: A new vulnerable source code dataset for deep learning based vulnerability detection},
  author={Chen, Yizheng and Ding, Zhoujie and Alowain, Lamya and Chen, Xinyun and Wagner, David},
  booktitle={Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
  pages={654--668},
  year={2023}
}
</code></pre>
