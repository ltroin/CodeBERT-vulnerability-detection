# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
import torch
import torch.nn as nn
import torch
from torch.autograd import Variable
import copy
from torch.nn import CrossEntropyLoss, MSELoss,BCEWithLogitsLoss

    
    
class Model(nn.Module):   
    def __init__(self, encoder,config,tokenizer,args):
        super(Model, self).__init__()
        self.encoder = encoder
        self.config=config
        self.tokenizer=tokenizer
        self.args=args
    
        # Define dropout layer, dropout_probability is taken from args.
        self.dropout = nn.Dropout(args.dropout_probability)

        
    def forward(self, input_ids=None,labels=None): 
        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]
        label_values = None
        if labels is not None:
            # Convert the set into a binary label set.
            label_values = torch.zeros(labels.size(0), 2)
            label_values[labels == 0, 0] = 1
            label_values[labels == 1, 1] = 1
            
            # Apply dropout
            outputs = self.dropout(outputs)
            # Ensure label_values is on the correct device
            label_values = label_values.to(self.args.device)


        # loss(input_ids, outputs)
        logits=outputs
        prob=torch.sigmoid(logits)
        if labels is not None:
            if self.args.loss_func == "BCEWithLogits":
                labels=labels.float()
                loss_value= (torch.log(prob[:,0]+1e-10)*labels*self.args.BCE_bias+torch.log((1-prob)[:,0]+1e-10)*(1-labels))
                loss_value=-loss_value.mean()
            elif self.args.loss_func == "CrossEntropy":
                loss_value = nn.CrossEntropyLoss()(outputs, labels)
            elif self.args.loss_func == "MSE":
                loss_value = nn.MSELoss()(outputs, label_values)
            elif self.args.loss_func == "BCE":
                loss_value = nn.BCELoss()(prob, label_values)
            else:
                labels=labels.float()
                loss_value=torch.log(prob[:,0])*labels+torch.log((1-prob)[:,0])*(1-labels)
                loss_value=-loss_value.mean()
            return loss_value,prob
        else:
            return prob
      
        
 
