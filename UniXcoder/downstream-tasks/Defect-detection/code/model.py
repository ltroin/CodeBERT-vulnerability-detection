# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
import torch
import torch.nn as nn
import torch
from torch.autograd import Variable
import copy
from torch.nn import CrossEntropyLoss, MSELoss,BCEWithLogitsLoss

    
    
class Model(nn.Module):  
    """
    This class implements a custom model which is primarily designed to work with an encoder for a specific task. 
    The encoder will be trained is the UnixCoder or any that outputs a sequence of hidden-states as its first output.

    Attributes:
        encoder (nn.Module): A pre-defined encoder model (like BERT, GPT, etc.) which provides the main functionality.
        config: Configuration object specific to the encoder, containing various attributes about the model.
        tokenizer: Tokenizer object for converting text input into a format that is compatible with the encoder.
        args: ArgumentParser object or similar structure containing various hyperparameters and options for the model.
        dropout (nn.Dropout): A dropout layer for regularizing the model to prevent overfitting.

    Args:
        encoder (nn.Module): The encoder neural network model to be used.
        config: Configuration details specific to the encoder.
        tokenizer: The tokenizer to pre-process input data for the encoder.
        args: Command line arguments or other configurations.

    Methods:
        forward(input_ids=None, labels=None):
            Processes input data through the encoder, applies dropout, and computes loss if labels are provided.

            Args:
                input_ids (Tensor): Tensor containing sequences of input IDs to be processed by the model.
                labels (Tensor): Tensor containing the labels for the input sequences for supervised training.

            Returns:
                Tensor: If labels are not None, it returns a tuple (loss_value, prob), where `loss_value` is 
                a tensor containing the loss calculated based on the specified loss function and `prob` 
                contains the model probabilities. If labels are None, it returns `prob` only.
    """ 
    def __init__(self, encoder,config,tokenizer,args):
        super(Model, self).__init__()
        self.encoder = encoder
        self.config=config
        self.tokenizer=tokenizer
        self.args=args
    
        # Define dropout layer, dropout_probability is taken from args.
        self.dropout = nn.Dropout(args.dropout_probability)

        
    def forward(self, input_ids=None, labels=None):
        # Process the input through the encoder with attention masking (ignoring padding tokens, assumed to be indexed by 1).
        outputs = self.encoder(input_ids, attention_mask=input_ids.ne(1))[0]
        
        label_values = None
        
        # If labels are provided, proceed to process them for loss computation.
        if labels is not None:
            # Initialize a tensor to hold binary label values for a binary classification task.
            label_values = torch.zeros(labels.size(0), 2)
            # For a label of 0, set the first column to 1, and for a label of 1, set the second column to 1.
            label_values[labels == 0, 0] = 1
            label_values[labels == 1, 1] = 1
            
            # Apply dropout to the encoder outputs for regularization.
            outputs = self.dropout(outputs)
            # Move the binary label tensor to the same device as the model (e.g., CPU or GPU).
            label_values = label_values.to(self.args.device)

        # The logits are simply the outputs of the encoder after dropout.
        logits = outputs
        # Apply the sigmoid function to the logits to get probabilities between 0 and 1.
        prob = torch.sigmoid(logits)
        
        # If labels were provided, compute the loss.
        if labels is not None:
            # Check which loss function to use based on the arguments.
            if self.args.loss_func == "BCEWithLogits":
                # Convert labels to float for BCEWithLogitsLoss calculation.
                labels = labels.float()
                # Calculate the loss manually with bias term for positive examples.
                loss_value = (torch.log(prob[:,0]+1e-10)*labels*self.args.BCE_bias +
                            torch.log((1-prob)[:,0]+1e-10)*(1-labels))
                # Negate the result to get the positive loss value.
                loss_value = -loss_value.mean()
            elif self.args.loss_func == "BCEWithLogitsWeight":
                labels = labels.float()
                loss_value = self.args.BCE_bias *(torch.log(prob[:,0]+1e-10)*labels+
                            torch.log((1-prob)[:,0]+1e-10)*(1-labels))
                loss_value = -loss_value.mean()
            elif self.args.loss_func == "CrossEntropy":
                loss_value = nn.CrossEntropyLoss()(outputs, labels)
            elif self.args.loss_func == "MSE":
                loss_value = nn.MSELoss()(outputs, label_values)
            elif self.args.loss_func == "BCE":
                loss_value = nn.BCELoss()(prob, label_values)
            else:
                # If no specific loss function is provided, compute a simple binary cross-entropy loss.
                loss_value = torch.log(prob[:,0])*labels + torch.log((1-prob)[:,0])*(1-labels)
                # Again, negate the result to get the positive loss value.
                loss_value = -loss_value.mean()
            # Return the loss value along with the probabilities.
            return loss_value, prob
        else:
            # If no labels are provided, just return the probabilities.
            return prob

      
        
 
