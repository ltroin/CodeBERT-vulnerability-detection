{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14KzsZyhGJy7",
    "outputId": "c2b75351-6164-4c09-a5b5-18c56cded20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.17.3 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/d0/0b/26ad95cf0b747be967b15fb71a06f5ac67aba0fd2f9cd174de6edefc4674/scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/18/44/7e8d208eb59a8224fcc474415104f13be9b378be8da63f76dfde12ec2b44/scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.11.3 threadpoolctl-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.26.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (23.1)\n",
      "Collecting protobuf>=3.20 (from tensorboardX)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/c8/2c/03046cac73f46bfe98fc846ef629cf4f84c2f59258216aa2cc0d22bfca8f/protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-4.24.4 tensorboardX-2.6.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 29 19:32:54 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              16W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  | 00000000:02:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              15W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  | 00000000:03:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              19W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  | 00000000:04:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              20W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               On  | 00000000:05:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              19W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               On  | 00000000:06:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              25W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               On  | 00000000:07:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              20W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000               On  | 00000000:08:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              29W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study the usage of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/29/2023 20:18:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/29/2023 20:18:56 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='MSE', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   idx: 0\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   label: 0\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'da', 'dec', '_', 'init', '(', 'AV', 'CodecContext', '_*', 'avctx', ')', '_{', '_V', 'DA', 'Decoder', 'Context', '_*', 'ctx', '_=', '_avctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_ctx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_pix', '_', 'fmts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', ')', '_{', '_if', '_(', 'k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '_<', '_k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', '_', 'prior', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_memset', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_avctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_avctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', \"_'\", 'avc', '1', \"';\", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'decoding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_ctx', '->', 'pix', '_', 'fmt', '_=', '_avctx', '->', 'get', '_', 'format', '(', 'avctx', ',', '_avctx', '->', 'codec', '->', 'pix', '_', 'fmts', ');', '_switch', '_(', 'ctx', '->', 'pix', '_', 'fmt', ')', '_{', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'UY', 'VY', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '2', 'vu', 'y', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YU', 'YV', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'yu', 'vs', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'NV', '12', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '420', 'v', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YUV', '420', 'P', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'y', '420', \"';\", '_break', ';', '_default', ':', '_av', '_', 'log', '(', 'avctx', ',', '_AV', '</s>']\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   input_ids: 0 932 2729 181 37557 554 460 2446 3418 181 1242 126 4543 40294 426 16581 127 399 1010 4910 6706 1133 426 1203 385 24237 408 996 181 636 145 1277 460 2446 181 1499 426 204 2446 181 1203 385 519 1203 408 204 2446 181 1203 145 5027 1536 2104 145 554 1234 145 3025 408 190 12383 181 9617 385 461 145 1067 1796 9718 181 49749 595 10502 634 462 802 544 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 127 399 462 400 193 3196 3699 21876 1877 1934 517 1085 3196 3699 21876 1877 1934 1083 181 141 127 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 181 21982 181 1083 181 141 145 669 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 145 425 1067 1796 460 2446 634 9597 126 204 2446 181 1203 130 461 130 1710 126 627 460 2446 181 1499 648 460 2446 181 1203 408 1729 385 24237 408 1729 145 460 2446 181 1203 408 2047 385 24237 408 2047 145 460 2446 181 1203 408 1478 385 464 48271 135 1177 460 2446 181 1203 408 1031 181 3616 181 43974 385 524 145 460 2446 181 1203 408 1031 181 903 181 1601 385 524 145 3025 408 7009 181 2976 385 24237 408 459 181 1478 126 16581 130 24237 408 4403 408 7009 181 49749 388 2444 400 1203 408 7009 181 2976 127 399 883 7082 181 10526 181 9103 181 22440 20978 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 136 26233 207 1177 1127 145 883 7082 181 10526 181 9103 181 16006 26736 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 36066 4728 1177 1127 145 883 7082 181 10526 181 9103 181 7337 1093 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 21034 204 1177 1127 145 883 7082 181 10526 181 9103 181 19872 21034 166 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 207 21034 1177 1127 145 1361 144 2729 181 896 126 16581 130 7082 2\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   idx: 1\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   label: 0\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_int', '_trans', 'code', '(', 'AV', 'Format', 'Context', '_**', 'output', '_', 'files', ',', '_int', '_nb', '_', 'output', '_', 'files', ',', '_Input', 'File', '_*', 'input', '_', 'files', ',', '_int', '_nb', '_', 'input', '_', 'files', ',', '_Stream', 'Map', '_*', 'stream', '_', 'maps', ',', '_int', '_nb', '_', 'stream', '_', 'maps', ')', '_{', '_int', '_ret', '_=', '_0', ',', '_i', ',', '_j', ',', '_k', ',', '_n', ',', '_nb', '_', 'ostream', 's', '_=', '_0', ',', '_step', ';', '_AV', 'Format', 'Context', '_*', 'is', ',', '_*', 'os', ';', '_AV', 'CodecContext', '_*', 'codec', ',', '_*', 'icode', 'c', ';', '_OutputStream', '_*', 'ost', ',', '_**', 'ost', '_', 'table', '_=', '_NULL', ';', '_InputStream', '_*', 'ist', ';', '_char', '_error', '[', '1024', '];', '_int', '_key', ';', '_int', '_want', '_', 'sdp', '_=', '_1', ';', '_uint', '8', '_', 't', '_no', '_', 'packet', '[', 'MAX', '_', 'FILES', ']={', '0', '};', '_int', '_no', '_', 'packet', '_', 'count', '=', '0', ';', '_int', '_nb', '_', 'frame', '_', 'threshold', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_int', '_nb', '_', 'streams', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_if', '_(', 'rate', '_', 'emu', ')', '_for', '_(', 'i', '_=', '_0', ';', '_i', '_<', '_nb', '_', 'input', '_', 'streams', ';', '_i', '++)', '_input', '_', 'streams', '[', 'i', '].', 'start', '_=', '_av', '_', 'gettime', '();', '_/*', '_output', '_stream', '_init', '_*/', '_nb', '_', 'ostream', 's', '_=', '_0', ';', '_for', '(', 'i', '=', '0', ';', 'i', '<', 'nb', '_', 'output', '_', 'files', ';', 'i', '++)', '_{', '_os', '_=', '_output', '_', 'files', '[', 'i', '];', '_if', '_(!', 'os', '->', 'nb', '_', 'streams', '_&&', '_!(', 'os', '->', 'o', 'format', '->', 'flags', '_&', '_A', 'VF', 'MT', '_', 'NO', 'STREAM', 'S', '))', '_{', '_av', '_', 'dump', '_', 'format', '(', 'output', '_', 'files', '[', 'i', '],', '_i', ',', '_output', '_', 'files', '[', 'i', ']->', 'filename', ',', '_1', ');', '_fprintf', '(', 'stderr', ',', '_\"', 'Output', '_file', '_#%', 'd', '_does', '_not', '_contain', '_any', '_stream', '\\\\', 'n', '\",', '_i', ');', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_nb', '_', 'ostream', 's', '_+=', '_os', '->', 'nb', '_', 'streams', ';', '_}', '_if', '_(', 'nb', '_', 'stream', '_', 'maps', '_>', '_0', '_&&', '_nb', '_', 'stream', '_', 'maps', '_!=', '_nb', '_', 'ostream', 's', ')', '_{', '_fprintf', '(', 'stderr', ',', '_\"', 'Number', '_of', '_stream', '_maps', '_must', '_match', '_number', '_of', '_output', '_streams', '\\\\', 'n', '\");', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_/*', '_Sanity', '_check', '_the', '_mapping', '_args', '_--', '_do', '_the', '_input', '_files', '_&', '_streams', '_exist', '?', '</s>']\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   input_ids: 0 932 554 1688 780 126 4543 1660 1133 2760 1788 181 3011 130 554 9091 181 1788 181 3011 130 4715 956 426 1376 181 3011 130 554 9091 181 1376 181 3011 130 7141 1281 426 1719 181 9865 130 554 9091 181 1719 181 9865 127 399 554 1234 385 461 130 548 130 913 130 1085 130 416 130 9091 181 10980 201 385 461 130 4091 145 7082 1660 1133 426 402 130 426 654 145 7082 40294 426 4403 130 426 3888 185 145 33734 426 804 130 2760 804 181 1389 385 1008 145 17932 426 507 145 1108 843 177 8681 823 554 1129 145 554 2947 181 15425 385 524 145 941 142 181 202 1375 181 4017 177 2485 181 16930 29435 134 1441 554 1375 181 4017 181 1048 147 134 145 554 9091 181 1966 181 8595 177 4543 15828 181 1850 181 11963 29435 134 1441 554 9091 181 13250 177 4543 15828 181 1850 181 11963 29435 134 1441 462 400 2779 181 9145 127 563 400 191 385 461 145 548 517 9091 181 1376 181 13250 145 548 1232 1586 181 13250 177 191 859 1094 385 2729 181 45105 523 1067 1721 2239 1796 634 9091 181 10980 201 385 461 145 563 126 191 147 134 145 191 146 4269 181 1788 181 3011 145 191 1232 399 2215 385 1721 181 3011 177 191 823 462 802 654 408 4269 181 13250 698 6943 654 408 197 1478 408 1550 519 553 6644 3563 181 2477 8129 169 509 399 2729 181 4617 181 1478 126 1788 181 3011 177 191 759 548 130 1721 181 3011 177 191 3703 2473 130 524 388 5800 126 4691 130 437 2203 1012 19003 186 2129 800 2910 1800 2239 178 196 475 548 388 1234 385 22451 126 3298 388 4484 2269 145 425 9091 181 10980 201 1054 2215 408 4269 181 13250 145 425 462 400 4269 181 1719 181 9865 711 461 698 9091 181 1719 181 9865 620 9091 181 10980 201 127 399 5800 126 4691 130 437 1934 595 2239 12176 2016 1655 1635 595 1721 16323 178 196 659 1234 385 22451 126 3298 388 4484 2269 145 425 1067 29011 1382 448 4860 1822 2850 1000 448 1586 2966 519 16323 3040 149 2\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   idx: 2\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   label: 0\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_void', '_v', '4', 'l', '2', '_', 'free', '_', 'buffer', '(', 'void', '_*', 'opaque', ',', '_uint', '8', '_', 't', '_*', 'unused', ')', '_{', '_V', '4', 'L', '2', 'Buffer', '*', '_av', 'buf', '_=', '_opaque', ';', '_V', '4', 'L', '2', 'm', '2', 'mContext', '_*', 's', '_=', '_buf', '_', 'to', '_', 'm', '2', 'mctx', '(', 'av', 'buf', ');', '_if', '_(', 'atomic', '_', 'fetch', '_', 'sub', '(&', 'av', 'buf', '->', 'context', '_', 'refcount', ',', '_1', ')', '_==', '_1', ')', '_{', '_atomic', '_', 'fetch', '_', 'sub', '_', 'explicit', '(&', 's', '->', 'refcount', ',', '_1', ',', '_memory', '_', 'order', '_', 'ac', 'q', '_', 'rel', ');', '_if', '_(', 's', '->', 'reinit', ')', '_{', '_if', '_(!', 'atomic', '_', 'load', '(&', 's', '->', 'refcount', '))', '_sem', '_', 'post', '(&', 's', '->', 'ref', 'sync', ');', '_}', '_else', '_if', '_(', 'av', 'buf', '->', 'context', '->', 'stream', 'on', ')', '_ff', '_', 'v', '4', 'l', '2', '_', 'buffer', '_', 'enqueue', '(', 'av', 'buf', ');', '_av', '_', 'buffer', '_', 'unref', '(&', 'av', 'buf', '->', 'context', '_', 'ref', ');', '_}', '_}', '</s>']\n",
      "10/29/2023 20:19:37 - INFO - __main__ -   input_ids: 0 932 723 460 138 194 136 181 1654 181 1601 126 895 426 12458 130 941 142 181 202 426 9983 127 399 1010 138 162 136 1579 128 2729 1091 385 18128 145 1010 138 162 136 195 136 24943 426 201 385 2200 181 589 181 195 136 40828 126 1070 1091 388 462 400 6037 181 5011 181 1175 862 1070 1091 408 1499 181 16660 130 524 127 550 524 127 399 10285 181 5011 181 1175 181 16784 862 201 408 16660 130 524 130 3338 181 1346 181 921 199 181 2526 388 462 400 201 408 41486 127 399 462 802 6037 181 1030 862 201 408 16660 509 10254 181 1892 862 201 408 903 3616 388 425 669 462 400 1070 1091 408 1499 408 1719 382 127 11384 181 204 138 194 136 181 1601 181 13226 126 1070 1091 388 2729 181 1601 181 18327 862 1070 1091 408 1499 181 903 388 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/29/2023 20:19:38 - INFO - __main__ -   ***** Running training *****\n",
      "10/29/2023 20:19:38 - INFO - __main__ -     Num examples = 21854\n",
      "10/29/2023 20:19:38 - INFO - __main__ -     Num Epochs = 3\n",
      "10/29/2023 20:19:38 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/29/2023 20:19:38 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/29/2023 20:19:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/29/2023 20:19:38 - INFO - __main__ -     Total optimization steps = 2049\n",
      "  0%|          | 0/683 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.33998:  10%|▉         | 67/683 [01:03<08:51,  1.16it/s] 10/29/2023 20:20:46 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:20:46 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:20:46 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:21:05 - INFO - __main__ -     eval_loss = 0.2602\n",
      "10/29/2023 20:21:05 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:21:05 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "10/29/2023 20:21:05 - INFO - __main__ -     ********************\n",
      "10/29/2023 20:21:05 - INFO - __main__ -     Best acc:0.4345\n",
      "10/29/2023 20:21:05 - INFO - __main__ -     ********************\n",
      "10/29/2023 20:21:06 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/29/2023 20:21:06 - INFO - __main__ -     ********************\n",
      "10/29/2023 20:21:06 - INFO - __main__ -     Best f1:0.6058\n",
      "10/29/2023 20:21:06 - INFO - __main__ -     ********************\n",
      "10/29/2023 20:21:07 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.33998:  10%|▉         | 68/683 [01:29<1:28:39,  8.65s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.30582:  20%|█▉        | 135/683 [02:27<07:46,  1.17it/s] 10/29/2023 20:22:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:22:10 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:22:10 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:22:29 - INFO - __main__ -     eval_loss = 0.2963\n",
      "10/29/2023 20:22:29 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:22:29 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.30582:  20%|█▉        | 136/683 [02:51<1:13:49,  8.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.29603:  30%|██▉       | 203/683 [03:50<06:51,  1.17it/s]  10/29/2023 20:23:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:23:33 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:23:33 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:23:53 - INFO - __main__ -     eval_loss = 0.275\n",
      "10/29/2023 20:23:53 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:23:53 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.29603:  30%|██▉       | 204/683 [04:14<1:04:56,  8.13s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.28625:  40%|███▉      | 271/683 [05:13<05:53,  1.16it/s]  10/29/2023 20:24:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:24:57 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:24:57 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:25:16 - INFO - __main__ -     eval_loss = 0.2465\n",
      "10/29/2023 20:25:16 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:25:16 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.28625:  40%|███▉      | 272/683 [05:38<55:37,  8.12s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.28035:  50%|████▉     | 339/683 [06:37<04:56,  1.16it/s]10/29/2023 20:26:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:26:20 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:26:20 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:26:39 - INFO - __main__ -     eval_loss = 0.2729\n",
      "10/29/2023 20:26:39 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:26:39 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.28035:  50%|████▉     | 340/683 [07:01<46:22,  8.11s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.27601:  60%|█████▉    | 407/683 [08:00<03:58,  1.16it/s]10/29/2023 20:27:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:27:43 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:27:43 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:28:02 - INFO - __main__ -     eval_loss = 0.2387\n",
      "10/29/2023 20:28:02 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:28:02 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.27601:  60%|█████▉    | 408/683 [08:24<37:22,  8.15s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.2716:  70%|██████▉   | 475/683 [09:23<03:00,  1.15it/s] 10/29/2023 20:29:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:29:06 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:29:06 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:29:26 - INFO - __main__ -     eval_loss = 0.2355\n",
      "10/29/2023 20:29:26 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:29:26 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.2716:  70%|██████▉   | 476/683 [09:47<28:01,  8.12s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26809:  80%|███████▉  | 543/683 [10:47<02:00,  1.16it/s]10/29/2023 20:30:30 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:30:30 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:30:30 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:30:49 - INFO - __main__ -     eval_loss = 0.2293\n",
      "10/29/2023 20:30:49 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 20:30:49 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "epoch 0 loss 0.26809:  80%|███████▉  | 544/683 [11:11<18:46,  8.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26511:  89%|████████▉ | 611/683 [12:10<01:01,  1.17it/s]10/29/2023 20:31:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:31:53 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:31:53 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:32:12 - INFO - __main__ -     eval_loss = 0.2248\n",
      "10/29/2023 20:32:12 - INFO - __main__ -     eval_acc = 0.4305\n",
      "10/29/2023 20:32:12 - INFO - __main__ -     eval_f1 = 0.6018\n",
      "epoch 0 loss 0.26511:  90%|████████▉ | 612/683 [12:34<09:40,  8.17s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26218:  99%|█████████▉| 679/683 [13:33<00:03,  1.14it/s]10/29/2023 20:33:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:33:16 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:33:16 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:33:35 - INFO - __main__ -     eval_loss = 0.2241\n",
      "10/29/2023 20:33:35 - INFO - __main__ -     eval_acc = 0.4279\n",
      "10/29/2023 20:33:35 - INFO - __main__ -     eval_f1 = 0.5993\n",
      "epoch 0 loss 0.26218: 100%|█████████▉| 680/683 [13:57<00:24,  8.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26198: 100%|██████████| 683/683 [13:59<00:00,  1.23s/it]\n",
      "epoch 1 loss 0.22256:   9%|▉         | 64/683 [00:57<09:18,  1.11it/s]10/29/2023 20:34:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:34:40 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:34:40 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:34:59 - INFO - __main__ -     eval_loss = 0.2211\n",
      "10/29/2023 20:34:59 - INFO - __main__ -     eval_acc = 0.4154\n",
      "10/29/2023 20:34:59 - INFO - __main__ -     eval_f1 = 0.5864\n",
      "epoch 1 loss 0.22256:  10%|▉         | 65/683 [01:21<1:24:09,  8.17s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.22299:  19%|█▉        | 132/683 [02:20<08:11,  1.12it/s] 10/29/2023 20:36:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:36:03 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:36:03 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:36:22 - INFO - __main__ -     eval_loss = 0.2177\n",
      "10/29/2023 20:36:22 - INFO - __main__ -     eval_acc = 0.4305\n",
      "10/29/2023 20:36:22 - INFO - __main__ -     eval_f1 = 0.6018\n",
      "epoch 1 loss 0.22299:  19%|█▉        | 133/683 [02:44<1:14:27,  8.12s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.22366:  29%|██▉       | 200/683 [03:43<07:07,  1.13it/s]  10/29/2023 20:37:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:37:26 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:37:26 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:37:45 - INFO - __main__ -     eval_loss = 0.2376\n",
      "10/29/2023 20:37:45 - INFO - __main__ -     eval_acc = 0.433\n",
      "10/29/2023 20:37:45 - INFO - __main__ -     eval_f1 = 0.6043\n",
      "epoch 1 loss 0.22366:  29%|██▉       | 201/683 [04:07<1:06:01,  8.22s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.22296:  39%|███▉      | 268/683 [05:06<05:55,  1.17it/s]  10/29/2023 20:38:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:38:49 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:38:49 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:39:08 - INFO - __main__ -     eval_loss = 0.2251\n",
      "10/29/2023 20:39:08 - INFO - __main__ -     eval_acc = 0.4096\n",
      "10/29/2023 20:39:08 - INFO - __main__ -     eval_f1 = 0.5796\n",
      "epoch 1 loss 0.22296:  39%|███▉      | 269/683 [05:30<55:45,  8.08s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.2219:  49%|████▉     | 336/683 [06:29<04:56,  1.17it/s] 10/29/2023 20:40:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:40:11 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:40:11 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:40:31 - INFO - __main__ -     eval_loss = 0.2155\n",
      "10/29/2023 20:40:31 - INFO - __main__ -     eval_acc = 0.429\n",
      "10/29/2023 20:40:31 - INFO - __main__ -     eval_f1 = 0.6004\n",
      "epoch 1 loss 0.2219:  49%|████▉     | 337/683 [06:53<46:20,  8.04s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.22066:  59%|█████▉    | 404/683 [07:52<03:58,  1.17it/s]10/29/2023 20:41:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:41:35 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:41:35 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:41:54 - INFO - __main__ -     eval_loss = 0.2143\n",
      "10/29/2023 20:41:54 - INFO - __main__ -     eval_acc = 0.4213\n",
      "10/29/2023 20:41:54 - INFO - __main__ -     eval_f1 = 0.5926\n",
      "epoch 1 loss 0.22066:  59%|█████▉    | 405/683 [08:16<37:29,  8.09s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.2205:  69%|██████▉   | 472/683 [09:14<02:59,  1.18it/s] 10/29/2023 20:42:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:42:57 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:42:57 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:43:16 - INFO - __main__ -     eval_loss = 0.214\n",
      "10/29/2023 20:43:16 - INFO - __main__ -     eval_acc = 0.4191\n",
      "10/29/2023 20:43:16 - INFO - __main__ -     eval_f1 = 0.5905\n",
      "epoch 1 loss 0.2205:  69%|██████▉   | 473/683 [09:38<28:15,  8.07s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.2202:  79%|███████▉  | 540/683 [10:37<02:02,  1.17it/s] 10/29/2023 20:44:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:44:20 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:44:20 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:44:39 - INFO - __main__ -     eval_loss = 0.2117\n",
      "10/29/2023 20:44:39 - INFO - __main__ -     eval_acc = 0.418\n",
      "10/29/2023 20:44:39 - INFO - __main__ -     eval_f1 = 0.5896\n",
      "epoch 1 loss 0.2202:  79%|███████▉  | 541/683 [11:01<18:58,  8.02s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.21971:  89%|████████▉ | 608/683 [12:00<01:04,  1.17it/s]10/29/2023 20:45:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:45:43 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:45:43 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:46:02 - INFO - __main__ -     eval_loss = 0.2107\n",
      "10/29/2023 20:46:02 - INFO - __main__ -     eval_acc = 0.4111\n",
      "10/29/2023 20:46:02 - INFO - __main__ -     eval_f1 = 0.5824\n",
      "epoch 1 loss 0.21971:  89%|████████▉ | 609/683 [12:24<10:02,  8.14s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.21896:  99%|█████████▉| 676/683 [13:23<00:05,  1.17it/s]10/29/2023 20:47:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:47:06 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:47:06 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:47:26 - INFO - __main__ -     eval_loss = 0.2128\n",
      "10/29/2023 20:47:26 - INFO - __main__ -     eval_acc = 0.41\n",
      "10/29/2023 20:47:26 - INFO - __main__ -     eval_f1 = 0.5811\n",
      "epoch 1 loss 0.21896:  99%|█████████▉| 677/683 [13:48<00:48,  8.14s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.21879: 100%|██████████| 683/683 [13:53<00:00,  1.22s/it]\n",
      "epoch 2 loss 0.19959:   9%|▉         | 61/683 [00:53<08:56,  1.16it/s]10/29/2023 20:48:30 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:48:30 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:48:30 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:48:49 - INFO - __main__ -     eval_loss = 0.2126\n",
      "10/29/2023 20:48:49 - INFO - __main__ -     eval_acc = 0.4045\n",
      "10/29/2023 20:48:49 - INFO - __main__ -     eval_f1 = 0.5753\n",
      "epoch 2 loss 0.19959:   9%|▉         | 62/683 [01:17<1:22:57,  8.02s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.1977:  19%|█▉        | 129/683 [02:16<07:54,  1.17it/s]  10/29/2023 20:49:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:49:52 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:49:52 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:50:11 - INFO - __main__ -     eval_loss = 0.2159\n",
      "10/29/2023 20:50:11 - INFO - __main__ -     eval_acc = 0.399\n",
      "10/29/2023 20:50:11 - INFO - __main__ -     eval_f1 = 0.5693\n",
      "epoch 2 loss 0.1977:  19%|█▉        | 130/683 [02:40<1:13:28,  7.97s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19428:  29%|██▉       | 197/683 [03:38<06:56,  1.17it/s]  10/29/2023 20:51:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:51:15 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:51:15 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:51:34 - INFO - __main__ -     eval_loss = 0.2181\n",
      "10/29/2023 20:51:34 - INFO - __main__ -     eval_acc = 0.3935\n",
      "10/29/2023 20:51:34 - INFO - __main__ -     eval_f1 = 0.5636\n",
      "epoch 2 loss 0.19428:  29%|██▉       | 198/683 [04:02<1:05:07,  8.06s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19305:  39%|███▉      | 265/683 [05:02<06:17,  1.11it/s]  10/29/2023 20:52:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:52:38 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:52:38 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:52:57 - INFO - __main__ -     eval_loss = 0.2159\n",
      "10/29/2023 20:52:57 - INFO - __main__ -     eval_acc = 0.4052\n",
      "10/29/2023 20:52:57 - INFO - __main__ -     eval_f1 = 0.5765\n",
      "epoch 2 loss 0.19305:  39%|███▉      | 266/683 [05:26<56:30,  8.13s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19332:  49%|████▉     | 333/683 [06:24<05:01,  1.16it/s]10/29/2023 20:54:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:54:01 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:54:01 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:54:20 - INFO - __main__ -     eval_loss = 0.2142\n",
      "10/29/2023 20:54:20 - INFO - __main__ -     eval_acc = 0.3946\n",
      "10/29/2023 20:54:20 - INFO - __main__ -     eval_f1 = 0.5654\n",
      "epoch 2 loss 0.19332:  49%|████▉     | 334/683 [06:49<47:33,  8.18s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19232:  59%|█████▊    | 401/683 [07:48<04:03,  1.16it/s]10/29/2023 20:55:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:55:24 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:55:24 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:55:44 - INFO - __main__ -     eval_loss = 0.2162\n",
      "10/29/2023 20:55:44 - INFO - __main__ -     eval_acc = 0.396\n",
      "10/29/2023 20:55:44 - INFO - __main__ -     eval_f1 = 0.5662\n",
      "epoch 2 loss 0.19232:  59%|█████▉    | 402/683 [08:12<37:53,  8.09s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19256:  69%|██████▊   | 469/683 [09:10<03:02,  1.17it/s]10/29/2023 20:56:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:56:47 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:56:47 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:57:06 - INFO - __main__ -     eval_loss = 0.2145\n",
      "10/29/2023 20:57:06 - INFO - __main__ -     eval_acc = 0.3968\n",
      "10/29/2023 20:57:06 - INFO - __main__ -     eval_f1 = 0.5665\n",
      "epoch 2 loss 0.19256:  69%|██████▉   | 470/683 [09:35<29:02,  8.18s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19193:  79%|███████▊  | 537/683 [10:34<02:05,  1.17it/s]10/29/2023 20:58:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:58:10 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:58:10 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:58:29 - INFO - __main__ -     eval_loss = 0.2126\n",
      "10/29/2023 20:58:29 - INFO - __main__ -     eval_acc = 0.4023\n",
      "10/29/2023 20:58:29 - INFO - __main__ -     eval_f1 = 0.5731\n",
      "epoch 2 loss 0.19193:  79%|███████▉  | 538/683 [10:58<19:41,  8.15s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19155:  89%|████████▊ | 605/683 [11:56<01:06,  1.17it/s]10/29/2023 20:59:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 20:59:33 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 20:59:33 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 20:59:52 - INFO - __main__ -     eval_loss = 0.2135\n",
      "10/29/2023 20:59:52 - INFO - __main__ -     eval_acc = 0.4001\n",
      "10/29/2023 20:59:52 - INFO - __main__ -     eval_f1 = 0.5704\n",
      "epoch 2 loss 0.19155:  89%|████████▊ | 606/683 [12:20<10:18,  8.03s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19127:  99%|█████████▊| 673/683 [13:19<00:08,  1.12it/s]10/29/2023 21:00:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:00:56 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:00:56 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:01:15 - INFO - __main__ -     eval_loss = 0.2131\n",
      "10/29/2023 21:01:15 - INFO - __main__ -     eval_acc = 0.4008\n",
      "10/29/2023 21:01:15 - INFO - __main__ -     eval_f1 = 0.5716\n",
      "epoch 2 loss 0.19127:  99%|█████████▊| 674/683 [13:43<01:12,  8.04s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.19104: 100%|██████████| 683/683 [13:51<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func MSE\\\n",
    "    --BCE_bias 1.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/29/2023 21:04:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/29/2023 21:04:12 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/29/2023 21:04:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:04:18 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:04:18 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/29/2023 21:04:42 - INFO - __main__ -   ***** Eval results *****\n",
      "10/29/2023 21:04:42 - INFO - __main__ -     eval_acc = 0.4345\n",
      "10/29/2023 21:04:42 - INFO - __main__ -     eval_f1 = 0.6058\n",
      "10/29/2023 21:04:42 - INFO - __main__ -     eval_loss = 0.7798\n",
      "10/29/2023 21:04:48 - INFO - __main__ -   ***** Running Test *****\n",
      "10/29/2023 21:04:48 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:04:48 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 43/43 [00:18<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worse than random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.45937042459736455}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study the manual implementation of the bce bias function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/29/2023 21:05:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/29/2023 21:05:14 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   idx: 0\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   label: 0\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'da', 'dec', '_', 'init', '(', 'AV', 'CodecContext', '_*', 'avctx', ')', '_{', '_V', 'DA', 'Decoder', 'Context', '_*', 'ctx', '_=', '_avctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_ctx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_pix', '_', 'fmts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', ')', '_{', '_if', '_(', 'k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '_<', '_k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', '_', 'prior', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_memset', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_avctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_avctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', \"_'\", 'avc', '1', \"';\", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'decoding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_ctx', '->', 'pix', '_', 'fmt', '_=', '_avctx', '->', 'get', '_', 'format', '(', 'avctx', ',', '_avctx', '->', 'codec', '->', 'pix', '_', 'fmts', ');', '_switch', '_(', 'ctx', '->', 'pix', '_', 'fmt', ')', '_{', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'UY', 'VY', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '2', 'vu', 'y', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YU', 'YV', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'yu', 'vs', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'NV', '12', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '420', 'v', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YUV', '420', 'P', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'y', '420', \"';\", '_break', ';', '_default', ':', '_av', '_', 'log', '(', 'avctx', ',', '_AV', '</s>']\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   input_ids: 0 932 2729 181 37557 554 460 2446 3418 181 1242 126 4543 40294 426 16581 127 399 1010 4910 6706 1133 426 1203 385 24237 408 996 181 636 145 1277 460 2446 181 1499 426 204 2446 181 1203 385 519 1203 408 204 2446 181 1203 145 5027 1536 2104 145 554 1234 145 3025 408 190 12383 181 9617 385 461 145 1067 1796 9718 181 49749 595 10502 634 462 802 544 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 127 399 462 400 193 3196 3699 21876 1877 1934 517 1085 3196 3699 21876 1877 1934 1083 181 141 127 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 181 21982 181 1083 181 141 145 669 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 145 425 1067 1796 460 2446 634 9597 126 204 2446 181 1203 130 461 130 1710 126 627 460 2446 181 1499 648 460 2446 181 1203 408 1729 385 24237 408 1729 145 460 2446 181 1203 408 2047 385 24237 408 2047 145 460 2446 181 1203 408 1478 385 464 48271 135 1177 460 2446 181 1203 408 1031 181 3616 181 43974 385 524 145 460 2446 181 1203 408 1031 181 903 181 1601 385 524 145 3025 408 7009 181 2976 385 24237 408 459 181 1478 126 16581 130 24237 408 4403 408 7009 181 49749 388 2444 400 1203 408 7009 181 2976 127 399 883 7082 181 10526 181 9103 181 22440 20978 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 136 26233 207 1177 1127 145 883 7082 181 10526 181 9103 181 16006 26736 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 36066 4728 1177 1127 145 883 7082 181 10526 181 9103 181 7337 1093 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 21034 204 1177 1127 145 883 7082 181 10526 181 9103 181 19872 21034 166 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 207 21034 1177 1127 145 1361 144 2729 181 896 126 16581 130 7082 2\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   idx: 1\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   label: 0\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_int', '_trans', 'code', '(', 'AV', 'Format', 'Context', '_**', 'output', '_', 'files', ',', '_int', '_nb', '_', 'output', '_', 'files', ',', '_Input', 'File', '_*', 'input', '_', 'files', ',', '_int', '_nb', '_', 'input', '_', 'files', ',', '_Stream', 'Map', '_*', 'stream', '_', 'maps', ',', '_int', '_nb', '_', 'stream', '_', 'maps', ')', '_{', '_int', '_ret', '_=', '_0', ',', '_i', ',', '_j', ',', '_k', ',', '_n', ',', '_nb', '_', 'ostream', 's', '_=', '_0', ',', '_step', ';', '_AV', 'Format', 'Context', '_*', 'is', ',', '_*', 'os', ';', '_AV', 'CodecContext', '_*', 'codec', ',', '_*', 'icode', 'c', ';', '_OutputStream', '_*', 'ost', ',', '_**', 'ost', '_', 'table', '_=', '_NULL', ';', '_InputStream', '_*', 'ist', ';', '_char', '_error', '[', '1024', '];', '_int', '_key', ';', '_int', '_want', '_', 'sdp', '_=', '_1', ';', '_uint', '8', '_', 't', '_no', '_', 'packet', '[', 'MAX', '_', 'FILES', ']={', '0', '};', '_int', '_no', '_', 'packet', '_', 'count', '=', '0', ';', '_int', '_nb', '_', 'frame', '_', 'threshold', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_int', '_nb', '_', 'streams', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_if', '_(', 'rate', '_', 'emu', ')', '_for', '_(', 'i', '_=', '_0', ';', '_i', '_<', '_nb', '_', 'input', '_', 'streams', ';', '_i', '++)', '_input', '_', 'streams', '[', 'i', '].', 'start', '_=', '_av', '_', 'gettime', '();', '_/*', '_output', '_stream', '_init', '_*/', '_nb', '_', 'ostream', 's', '_=', '_0', ';', '_for', '(', 'i', '=', '0', ';', 'i', '<', 'nb', '_', 'output', '_', 'files', ';', 'i', '++)', '_{', '_os', '_=', '_output', '_', 'files', '[', 'i', '];', '_if', '_(!', 'os', '->', 'nb', '_', 'streams', '_&&', '_!(', 'os', '->', 'o', 'format', '->', 'flags', '_&', '_A', 'VF', 'MT', '_', 'NO', 'STREAM', 'S', '))', '_{', '_av', '_', 'dump', '_', 'format', '(', 'output', '_', 'files', '[', 'i', '],', '_i', ',', '_output', '_', 'files', '[', 'i', ']->', 'filename', ',', '_1', ');', '_fprintf', '(', 'stderr', ',', '_\"', 'Output', '_file', '_#%', 'd', '_does', '_not', '_contain', '_any', '_stream', '\\\\', 'n', '\",', '_i', ');', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_nb', '_', 'ostream', 's', '_+=', '_os', '->', 'nb', '_', 'streams', ';', '_}', '_if', '_(', 'nb', '_', 'stream', '_', 'maps', '_>', '_0', '_&&', '_nb', '_', 'stream', '_', 'maps', '_!=', '_nb', '_', 'ostream', 's', ')', '_{', '_fprintf', '(', 'stderr', ',', '_\"', 'Number', '_of', '_stream', '_maps', '_must', '_match', '_number', '_of', '_output', '_streams', '\\\\', 'n', '\");', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_/*', '_Sanity', '_check', '_the', '_mapping', '_args', '_--', '_do', '_the', '_input', '_files', '_&', '_streams', '_exist', '?', '</s>']\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   input_ids: 0 932 554 1688 780 126 4543 1660 1133 2760 1788 181 3011 130 554 9091 181 1788 181 3011 130 4715 956 426 1376 181 3011 130 554 9091 181 1376 181 3011 130 7141 1281 426 1719 181 9865 130 554 9091 181 1719 181 9865 127 399 554 1234 385 461 130 548 130 913 130 1085 130 416 130 9091 181 10980 201 385 461 130 4091 145 7082 1660 1133 426 402 130 426 654 145 7082 40294 426 4403 130 426 3888 185 145 33734 426 804 130 2760 804 181 1389 385 1008 145 17932 426 507 145 1108 843 177 8681 823 554 1129 145 554 2947 181 15425 385 524 145 941 142 181 202 1375 181 4017 177 2485 181 16930 29435 134 1441 554 1375 181 4017 181 1048 147 134 145 554 9091 181 1966 181 8595 177 4543 15828 181 1850 181 11963 29435 134 1441 554 9091 181 13250 177 4543 15828 181 1850 181 11963 29435 134 1441 462 400 2779 181 9145 127 563 400 191 385 461 145 548 517 9091 181 1376 181 13250 145 548 1232 1586 181 13250 177 191 859 1094 385 2729 181 45105 523 1067 1721 2239 1796 634 9091 181 10980 201 385 461 145 563 126 191 147 134 145 191 146 4269 181 1788 181 3011 145 191 1232 399 2215 385 1721 181 3011 177 191 823 462 802 654 408 4269 181 13250 698 6943 654 408 197 1478 408 1550 519 553 6644 3563 181 2477 8129 169 509 399 2729 181 4617 181 1478 126 1788 181 3011 177 191 759 548 130 1721 181 3011 177 191 3703 2473 130 524 388 5800 126 4691 130 437 2203 1012 19003 186 2129 800 2910 1800 2239 178 196 475 548 388 1234 385 22451 126 3298 388 4484 2269 145 425 9091 181 10980 201 1054 2215 408 4269 181 13250 145 425 462 400 4269 181 1719 181 9865 711 461 698 9091 181 1719 181 9865 620 9091 181 10980 201 127 399 5800 126 4691 130 437 1934 595 2239 12176 2016 1655 1635 595 1721 16323 178 196 659 1234 385 22451 126 3298 388 4484 2269 145 425 1067 29011 1382 448 4860 1822 2850 1000 448 1586 2966 519 16323 3040 149 2\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   idx: 2\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   label: 0\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_void', '_v', '4', 'l', '2', '_', 'free', '_', 'buffer', '(', 'void', '_*', 'opaque', ',', '_uint', '8', '_', 't', '_*', 'unused', ')', '_{', '_V', '4', 'L', '2', 'Buffer', '*', '_av', 'buf', '_=', '_opaque', ';', '_V', '4', 'L', '2', 'm', '2', 'mContext', '_*', 's', '_=', '_buf', '_', 'to', '_', 'm', '2', 'mctx', '(', 'av', 'buf', ');', '_if', '_(', 'atomic', '_', 'fetch', '_', 'sub', '(&', 'av', 'buf', '->', 'context', '_', 'refcount', ',', '_1', ')', '_==', '_1', ')', '_{', '_atomic', '_', 'fetch', '_', 'sub', '_', 'explicit', '(&', 's', '->', 'refcount', ',', '_1', ',', '_memory', '_', 'order', '_', 'ac', 'q', '_', 'rel', ');', '_if', '_(', 's', '->', 'reinit', ')', '_{', '_if', '_(!', 'atomic', '_', 'load', '(&', 's', '->', 'refcount', '))', '_sem', '_', 'post', '(&', 's', '->', 'ref', 'sync', ');', '_}', '_else', '_if', '_(', 'av', 'buf', '->', 'context', '->', 'stream', 'on', ')', '_ff', '_', 'v', '4', 'l', '2', '_', 'buffer', '_', 'enqueue', '(', 'av', 'buf', ');', '_av', '_', 'buffer', '_', 'unref', '(&', 'av', 'buf', '->', 'context', '_', 'ref', ');', '_}', '_}', '</s>']\n",
      "10/29/2023 21:05:54 - INFO - __main__ -   input_ids: 0 932 723 460 138 194 136 181 1654 181 1601 126 895 426 12458 130 941 142 181 202 426 9983 127 399 1010 138 162 136 1579 128 2729 1091 385 18128 145 1010 138 162 136 195 136 24943 426 201 385 2200 181 589 181 195 136 40828 126 1070 1091 388 462 400 6037 181 5011 181 1175 862 1070 1091 408 1499 181 16660 130 524 127 550 524 127 399 10285 181 5011 181 1175 181 16784 862 201 408 16660 130 524 130 3338 181 1346 181 921 199 181 2526 388 462 400 201 408 41486 127 399 462 802 6037 181 1030 862 201 408 16660 509 10254 181 1892 862 201 408 903 3616 388 425 669 462 400 1070 1091 408 1499 408 1719 382 127 11384 181 204 138 194 136 181 1601 181 13226 126 1070 1091 388 2729 181 1601 181 18327 862 1070 1091 408 1499 181 903 388 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/29/2023 21:05:55 - INFO - __main__ -   ***** Running training *****\n",
      "10/29/2023 21:05:55 - INFO - __main__ -     Num examples = 21854\n",
      "10/29/2023 21:05:55 - INFO - __main__ -     Num Epochs = 3\n",
      "10/29/2023 21:05:55 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/29/2023 21:05:55 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/29/2023 21:05:55 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/29/2023 21:05:55 - INFO - __main__ -     Total optimization steps = 2049\n",
      "  0%|          | 0/683 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.70583:  10%|▉         | 67/683 [01:04<08:51,  1.16it/s] 10/29/2023 21:07:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:07:04 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:07:04 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:07:24 - INFO - __main__ -     eval_loss = 0.6832\n",
      "10/29/2023 21:07:24 - INFO - __main__ -     eval_acc = 0.5586\n",
      "10/29/2023 21:07:24 - INFO - __main__ -     eval_f1 = 0.2318\n",
      "10/29/2023 21:07:24 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:07:24 - INFO - __main__ -     Best acc:0.5586\n",
      "10/29/2023 21:07:24 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:07:25 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/29/2023 21:07:25 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:07:25 - INFO - __main__ -     Best f1:0.2318\n",
      "10/29/2023 21:07:25 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:07:25 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.70583:  10%|▉         | 68/683 [01:30<1:29:08,  8.70s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69837:  20%|█▉        | 135/683 [02:28<07:46,  1.17it/s] 10/29/2023 21:08:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:08:28 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:08:28 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:08:48 - INFO - __main__ -     eval_loss = 0.6818\n",
      "10/29/2023 21:08:48 - INFO - __main__ -     eval_acc = 0.5666\n",
      "10/29/2023 21:08:48 - INFO - __main__ -     eval_f1 = 0.1204\n",
      "10/29/2023 21:08:48 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:08:48 - INFO - __main__ -     Best acc:0.5666\n",
      "10/29/2023 21:08:48 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:08:48 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.69837:  20%|█▉        | 136/683 [02:53<1:16:07,  8.35s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69499:  30%|██▉       | 203/683 [03:52<06:53,  1.16it/s]  10/29/2023 21:09:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:09:52 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:09:52 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:10:11 - INFO - __main__ -     eval_loss = 0.6817\n",
      "10/29/2023 21:10:11 - INFO - __main__ -     eval_acc = 0.5549\n",
      "10/29/2023 21:10:11 - INFO - __main__ -     eval_f1 = 0.5352\n",
      "10/29/2023 21:10:11 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:10:11 - INFO - __main__ -     Best f1:0.5352\n",
      "10/29/2023 21:10:11 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:10:12 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.69499:  30%|██▉       | 204/683 [04:16<1:05:52,  8.25s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69304:  40%|███▉      | 271/683 [05:15<05:52,  1.17it/s]  10/29/2023 21:11:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:11:15 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:11:15 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:11:34 - INFO - __main__ -     eval_loss = 0.6846\n",
      "10/29/2023 21:11:34 - INFO - __main__ -     eval_acc = 0.5501\n",
      "10/29/2023 21:11:34 - INFO - __main__ -     eval_f1 = 0.5951\n",
      "10/29/2023 21:11:34 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:11:34 - INFO - __main__ -     Best f1:0.5951\n",
      "10/29/2023 21:11:34 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:11:35 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.69304:  40%|███▉      | 272/683 [05:40<57:24,  8.38s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69048:  50%|████▉     | 339/683 [06:39<04:58,  1.15it/s]10/29/2023 21:12:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:12:39 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:12:39 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:12:59 - INFO - __main__ -     eval_loss = 0.6842\n",
      "10/29/2023 21:12:59 - INFO - __main__ -     eval_acc = 0.5626\n",
      "10/29/2023 21:12:59 - INFO - __main__ -     eval_f1 = 0.6023\n",
      "10/29/2023 21:12:59 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:12:59 - INFO - __main__ -     Best f1:0.6023\n",
      "10/29/2023 21:12:59 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:12:59 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.69048:  50%|████▉     | 340/683 [07:04<47:48,  8.36s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.6875:  60%|█████▉    | 407/683 [08:03<03:58,  1.16it/s] 10/29/2023 21:14:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:14:03 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:14:03 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:14:22 - INFO - __main__ -     eval_loss = 0.6575\n",
      "10/29/2023 21:14:22 - INFO - __main__ -     eval_acc = 0.5787\n",
      "10/29/2023 21:14:22 - INFO - __main__ -     eval_f1 = 0.3588\n",
      "10/29/2023 21:14:22 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:14:22 - INFO - __main__ -     Best acc:0.5787\n",
      "10/29/2023 21:14:22 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:14:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.6875:  60%|█████▉    | 408/683 [08:28<37:45,  8.24s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.68298:  70%|██████▉   | 475/683 [09:26<02:57,  1.17it/s]10/29/2023 21:15:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:15:26 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:15:26 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:15:46 - INFO - __main__ -     eval_loss = 0.6605\n",
      "10/29/2023 21:15:46 - INFO - __main__ -     eval_acc = 0.5886\n",
      "10/29/2023 21:15:46 - INFO - __main__ -     eval_f1 = 0.5405\n",
      "10/29/2023 21:15:46 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:15:46 - INFO - __main__ -     Best acc:0.5886\n",
      "10/29/2023 21:15:46 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:15:46 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.68298:  70%|██████▉   | 476/683 [09:51<28:45,  8.33s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.6788:  80%|███████▉  | 543/683 [10:49<02:00,  1.16it/s] 10/29/2023 21:16:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:16:49 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:16:49 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:17:09 - INFO - __main__ -     eval_loss = 0.6266\n",
      "10/29/2023 21:17:09 - INFO - __main__ -     eval_acc = 0.6318\n",
      "10/29/2023 21:17:09 - INFO - __main__ -     eval_f1 = 0.5107\n",
      "10/29/2023 21:17:09 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:17:09 - INFO - __main__ -     Best acc:0.6318\n",
      "10/29/2023 21:17:09 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:17:09 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.6788:  80%|███████▉  | 544/683 [11:14<19:10,  8.28s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.67504:  89%|████████▉ | 611/683 [12:13<01:01,  1.16it/s]10/29/2023 21:18:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:18:13 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:18:13 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:18:32 - INFO - __main__ -     eval_loss = 0.622\n",
      "10/29/2023 21:18:32 - INFO - __main__ -     eval_acc = 0.6321\n",
      "10/29/2023 21:18:32 - INFO - __main__ -     eval_f1 = 0.4763\n",
      "10/29/2023 21:18:32 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:18:32 - INFO - __main__ -     Best acc:0.6321\n",
      "10/29/2023 21:18:32 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:18:33 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.67504:  90%|████████▉ | 612/683 [12:38<09:46,  8.27s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.67214:  99%|█████████▉| 679/683 [13:35<00:03,  1.18it/s]10/29/2023 21:19:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:19:35 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:19:35 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:19:54 - INFO - __main__ -     eval_loss = 0.6172\n",
      "10/29/2023 21:19:54 - INFO - __main__ -     eval_acc = 0.6387\n",
      "10/29/2023 21:19:54 - INFO - __main__ -     eval_f1 = 0.5381\n",
      "10/29/2023 21:19:54 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:19:54 - INFO - __main__ -     Best acc:0.6387\n",
      "10/29/2023 21:19:54 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:19:55 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.67214: 100%|█████████▉| 680/683 [14:00<00:24,  8.19s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.67193: 100%|██████████| 683/683 [14:02<00:00,  1.23s/it]\n",
      "epoch 1 loss 0.61492:   9%|▉         | 64/683 [00:56<09:06,  1.13it/s]10/29/2023 21:20:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:20:59 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:20:59 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:21:18 - INFO - __main__ -     eval_loss = 0.6202\n",
      "10/29/2023 21:21:18 - INFO - __main__ -     eval_acc = 0.6351\n",
      "10/29/2023 21:21:18 - INFO - __main__ -     eval_f1 = 0.5774\n",
      "epoch 1 loss 0.61492:  10%|▉         | 65/683 [01:20<1:23:23,  8.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.61339:  19%|█▉        | 132/683 [02:18<07:49,  1.17it/s] 10/29/2023 21:22:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:22:21 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:22:21 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:22:41 - INFO - __main__ -     eval_loss = 0.6085\n",
      "10/29/2023 21:22:41 - INFO - __main__ -     eval_acc = 0.6428\n",
      "10/29/2023 21:22:41 - INFO - __main__ -     eval_f1 = 0.4974\n",
      "10/29/2023 21:22:41 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:22:41 - INFO - __main__ -     Best acc:0.6428\n",
      "10/29/2023 21:22:41 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:22:42 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 1 loss 0.61339:  19%|█▉        | 133/683 [02:43<1:16:27,  8.34s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.61093:  29%|██▉       | 200/683 [03:42<06:50,  1.18it/s]  10/29/2023 21:23:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:23:45 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:23:45 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:24:04 - INFO - __main__ -     eval_loss = 0.6198\n",
      "10/29/2023 21:24:04 - INFO - __main__ -     eval_acc = 0.6288\n",
      "10/29/2023 21:24:04 - INFO - __main__ -     eval_f1 = 0.3935\n",
      "epoch 1 loss 0.61093:  29%|██▉       | 201/683 [04:06<1:04:01,  7.97s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.60819:  39%|███▉      | 268/683 [05:05<05:53,  1.17it/s]  10/29/2023 21:25:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:25:08 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:25:08 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:25:27 - INFO - __main__ -     eval_loss = 0.6256\n",
      "10/29/2023 21:25:27 - INFO - __main__ -     eval_acc = 0.6314\n",
      "10/29/2023 21:25:27 - INFO - __main__ -     eval_f1 = 0.6119\n",
      "10/29/2023 21:25:27 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:25:27 - INFO - __main__ -     Best f1:0.6119\n",
      "10/29/2023 21:25:27 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:25:27 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 1 loss 0.60819:  39%|███▉      | 269/683 [05:29<56:42,  8.22s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.6074:  49%|████▉     | 336/683 [06:28<04:54,  1.18it/s] 10/29/2023 21:26:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:26:31 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:26:31 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:26:50 - INFO - __main__ -     eval_loss = 0.604\n",
      "10/29/2023 21:26:50 - INFO - __main__ -     eval_acc = 0.6449\n",
      "10/29/2023 21:26:50 - INFO - __main__ -     eval_f1 = 0.4538\n",
      "10/29/2023 21:26:50 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:26:50 - INFO - __main__ -     Best acc:0.6449\n",
      "10/29/2023 21:26:50 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:26:51 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 1 loss 0.6074:  49%|████▉     | 337/683 [06:52<47:38,  8.26s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.60511:  59%|█████▉    | 404/683 [07:51<03:58,  1.17it/s]10/29/2023 21:27:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:27:54 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:27:54 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:28:13 - INFO - __main__ -     eval_loss = 0.6042\n",
      "10/29/2023 21:28:13 - INFO - __main__ -     eval_acc = 0.6523\n",
      "10/29/2023 21:28:13 - INFO - __main__ -     eval_f1 = 0.5992\n",
      "10/29/2023 21:28:13 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:28:13 - INFO - __main__ -     Best acc:0.6523\n",
      "10/29/2023 21:28:13 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:28:13 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 1 loss 0.60511:  59%|█████▉    | 405/683 [08:15<38:08,  8.23s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.60386:  69%|██████▉   | 472/683 [09:14<03:01,  1.16it/s]10/29/2023 21:29:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:29:17 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:29:17 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:29:36 - INFO - __main__ -     eval_loss = 0.608\n",
      "10/29/2023 21:29:36 - INFO - __main__ -     eval_acc = 0.6468\n",
      "10/29/2023 21:29:36 - INFO - __main__ -     eval_f1 = 0.5974\n",
      "epoch 1 loss 0.60386:  69%|██████▉   | 473/683 [09:38<28:15,  8.07s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.60307:  79%|███████▉  | 540/683 [10:36<02:02,  1.17it/s]10/29/2023 21:30:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:30:39 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:30:39 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:30:59 - INFO - __main__ -     eval_loss = 0.597\n",
      "10/29/2023 21:30:59 - INFO - __main__ -     eval_acc = 0.6548\n",
      "10/29/2023 21:30:59 - INFO - __main__ -     eval_f1 = 0.5278\n",
      "10/29/2023 21:30:59 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:30:59 - INFO - __main__ -     Best acc:0.6548\n",
      "10/29/2023 21:30:59 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:30:59 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 1 loss 0.60307:  79%|███████▉  | 541/683 [11:01<19:50,  8.38s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.60174:  89%|████████▉ | 608/683 [12:00<01:04,  1.17it/s]10/29/2023 21:32:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:32:03 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:32:03 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:32:22 - INFO - __main__ -     eval_loss = 0.5996\n",
      "10/29/2023 21:32:22 - INFO - __main__ -     eval_acc = 0.6552\n",
      "10/29/2023 21:32:22 - INFO - __main__ -     eval_f1 = 0.6268\n",
      "10/29/2023 21:32:22 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:32:22 - INFO - __main__ -     Best acc:0.6552\n",
      "10/29/2023 21:32:22 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:32:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/29/2023 21:32:23 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:32:23 - INFO - __main__ -     Best f1:0.6268\n",
      "10/29/2023 21:32:23 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:32:23 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 1 loss 0.60174:  89%|████████▉ | 609/683 [12:25<10:30,  8.52s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.59941:  99%|█████████▉| 676/683 [13:23<00:05,  1.18it/s]10/29/2023 21:33:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:33:26 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:33:26 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:33:45 - INFO - __main__ -     eval_loss = 0.5952\n",
      "10/29/2023 21:33:45 - INFO - __main__ -     eval_acc = 0.6545\n",
      "10/29/2023 21:33:45 - INFO - __main__ -     eval_f1 = 0.6275\n",
      "10/29/2023 21:33:45 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:33:45 - INFO - __main__ -     Best f1:0.6275\n",
      "10/29/2023 21:33:45 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:33:46 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 1 loss 0.59941:  99%|█████████▉| 677/683 [13:48<00:49,  8.21s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 loss 0.59893: 100%|██████████| 683/683 [13:53<00:00,  1.22s/it]\n",
      "epoch 2 loss 0.54414:   9%|▉         | 61/683 [00:53<08:53,  1.17it/s]10/29/2023 21:34:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:34:50 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:34:50 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:35:09 - INFO - __main__ -     eval_loss = 0.6112\n",
      "10/29/2023 21:35:09 - INFO - __main__ -     eval_acc = 0.6537\n",
      "10/29/2023 21:35:09 - INFO - __main__ -     eval_f1 = 0.6276\n",
      "10/29/2023 21:35:09 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:35:09 - INFO - __main__ -     Best f1:0.6276\n",
      "10/29/2023 21:35:09 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:35:10 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 2 loss 0.54414:   9%|▉         | 62/683 [01:17<1:24:59,  8.21s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.54171:  19%|█▉        | 129/683 [02:16<07:53,  1.17it/s] 10/29/2023 21:36:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:36:13 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:36:13 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:36:32 - INFO - __main__ -     eval_loss = 0.6062\n",
      "10/29/2023 21:36:32 - INFO - __main__ -     eval_acc = 0.6548\n",
      "10/29/2023 21:36:32 - INFO - __main__ -     eval_f1 = 0.6143\n",
      "epoch 2 loss 0.54171:  19%|█▉        | 130/683 [02:40<1:13:39,  7.99s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.53293:  29%|██▉       | 197/683 [03:39<06:57,  1.16it/s]  10/29/2023 21:37:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:37:36 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:37:36 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:37:55 - INFO - __main__ -     eval_loss = 0.6257\n",
      "10/29/2023 21:37:55 - INFO - __main__ -     eval_acc = 0.649\n",
      "10/29/2023 21:37:55 - INFO - __main__ -     eval_f1 = 0.6097\n",
      "epoch 2 loss 0.53293:  29%|██▉       | 198/683 [04:03<1:05:29,  8.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.52972:  39%|███▉      | 265/683 [05:01<05:56,  1.17it/s]  10/29/2023 21:38:58 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:38:58 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:38:58 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:39:17 - INFO - __main__ -     eval_loss = 0.6193\n",
      "10/29/2023 21:39:17 - INFO - __main__ -     eval_acc = 0.6567\n",
      "10/29/2023 21:39:17 - INFO - __main__ -     eval_f1 = 0.6049\n",
      "10/29/2023 21:39:17 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:39:17 - INFO - __main__ -     Best acc:0.6567\n",
      "10/29/2023 21:39:17 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:39:18 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 2 loss 0.52972:  39%|███▉      | 266/683 [05:26<58:02,  8.35s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.53025:  49%|████▉     | 333/683 [06:25<05:05,  1.15it/s]10/29/2023 21:40:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:40:22 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:40:22 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:40:41 - INFO - __main__ -     eval_loss = 0.6262\n",
      "10/29/2023 21:40:41 - INFO - __main__ -     eval_acc = 0.6446\n",
      "10/29/2023 21:40:41 - INFO - __main__ -     eval_f1 = 0.6214\n",
      "epoch 2 loss 0.53025:  49%|████▉     | 334/683 [06:49<47:05,  8.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.53039:  59%|█████▊    | 401/683 [07:47<04:02,  1.17it/s]10/29/2023 21:41:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:41:44 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:41:44 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:42:03 - INFO - __main__ -     eval_loss = 0.6116\n",
      "10/29/2023 21:42:03 - INFO - __main__ -     eval_acc = 0.6519\n",
      "10/29/2023 21:42:03 - INFO - __main__ -     eval_f1 = 0.6098\n",
      "epoch 2 loss 0.53039:  59%|█████▉    | 402/683 [08:11<37:31,  8.01s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.53036:  69%|██████▊   | 469/683 [09:09<03:02,  1.17it/s]10/29/2023 21:43:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:43:06 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:43:06 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:43:25 - INFO - __main__ -     eval_loss = 0.61\n",
      "10/29/2023 21:43:25 - INFO - __main__ -     eval_acc = 0.6526\n",
      "10/29/2023 21:43:25 - INFO - __main__ -     eval_f1 = 0.6175\n",
      "epoch 2 loss 0.53036:  69%|██████▉   | 470/683 [09:33<28:27,  8.02s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.52849:  79%|███████▊  | 537/683 [10:31<02:03,  1.18it/s]10/29/2023 21:44:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:44:29 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:44:29 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:44:48 - INFO - __main__ -     eval_loss = 0.6098\n",
      "10/29/2023 21:44:48 - INFO - __main__ -     eval_acc = 0.6537\n",
      "10/29/2023 21:44:48 - INFO - __main__ -     eval_f1 = 0.6088\n",
      "epoch 2 loss 0.52849:  79%|███████▉  | 538/683 [10:56<19:32,  8.09s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.52829:  89%|████████▊ | 605/683 [11:54<01:07,  1.16it/s]10/29/2023 21:45:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:45:51 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:45:51 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:46:10 - INFO - __main__ -     eval_loss = 0.6111\n",
      "10/29/2023 21:46:10 - INFO - __main__ -     eval_acc = 0.6559\n",
      "10/29/2023 21:46:10 - INFO - __main__ -     eval_f1 = 0.6106\n",
      "epoch 2 loss 0.52829:  89%|████████▊ | 606/683 [12:18<10:17,  8.02s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.52726:  99%|█████████▊| 673/683 [13:17<00:08,  1.15it/s]10/29/2023 21:47:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:47:14 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:47:14 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:47:34 - INFO - __main__ -     eval_loss = 0.611\n",
      "10/29/2023 21:47:34 - INFO - __main__ -     eval_acc = 0.6534\n",
      "10/29/2023 21:47:34 - INFO - __main__ -     eval_f1 = 0.6036\n",
      "epoch 2 loss 0.52726:  99%|█████████▊| 674/683 [13:42<01:13,  8.15s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 2 loss 0.52683: 100%|██████████| 683/683 [13:49<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func OriginalCrossEntropy\\\n",
    "    --BCE_bias 1.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/29/2023 21:47:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/29/2023 21:47:49 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/29/2023 21:47:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:47:55 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:47:55 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/29/2023 21:48:19 - INFO - __main__ -   ***** Eval results *****\n",
      "10/29/2023 21:48:19 - INFO - __main__ -     eval_acc = 0.6567\n",
      "10/29/2023 21:48:19 - INFO - __main__ -     eval_f1 = 0.6049\n",
      "10/29/2023 21:48:19 - INFO - __main__ -     eval_loss = 0.6193\n",
      "10/29/2023 21:48:25 - INFO - __main__ -   ***** Running Test *****\n",
      "10/29/2023 21:48:25 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:48:25 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 43/43 [00:18<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.6526354319180088}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/29/2023 21:56:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/29/2023 21:56:27 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='BCEWithLogits', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   idx: 0\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   label: 0\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'da', 'dec', '_', 'init', '(', 'AV', 'CodecContext', '_*', 'avctx', ')', '_{', '_V', 'DA', 'Decoder', 'Context', '_*', 'ctx', '_=', '_avctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_ctx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_pix', '_', 'fmts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', ')', '_{', '_if', '_(', 'k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '_<', '_k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', '_', 'prior', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_memset', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_avctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_avctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', \"_'\", 'avc', '1', \"';\", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'decoding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_ctx', '->', 'pix', '_', 'fmt', '_=', '_avctx', '->', 'get', '_', 'format', '(', 'avctx', ',', '_avctx', '->', 'codec', '->', 'pix', '_', 'fmts', ');', '_switch', '_(', 'ctx', '->', 'pix', '_', 'fmt', ')', '_{', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'UY', 'VY', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '2', 'vu', 'y', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YU', 'YV', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'yu', 'vs', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'NV', '12', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '420', 'v', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YUV', '420', 'P', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'y', '420', \"';\", '_break', ';', '_default', ':', '_av', '_', 'log', '(', 'avctx', ',', '_AV', '</s>']\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   input_ids: 0 932 2729 181 37557 554 460 2446 3418 181 1242 126 4543 40294 426 16581 127 399 1010 4910 6706 1133 426 1203 385 24237 408 996 181 636 145 1277 460 2446 181 1499 426 204 2446 181 1203 385 519 1203 408 204 2446 181 1203 145 5027 1536 2104 145 554 1234 145 3025 408 190 12383 181 9617 385 461 145 1067 1796 9718 181 49749 595 10502 634 462 802 544 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 127 399 462 400 193 3196 3699 21876 1877 1934 517 1085 3196 3699 21876 1877 1934 1083 181 141 127 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 181 21982 181 1083 181 141 145 669 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 145 425 1067 1796 460 2446 634 9597 126 204 2446 181 1203 130 461 130 1710 126 627 460 2446 181 1499 648 460 2446 181 1203 408 1729 385 24237 408 1729 145 460 2446 181 1203 408 2047 385 24237 408 2047 145 460 2446 181 1203 408 1478 385 464 48271 135 1177 460 2446 181 1203 408 1031 181 3616 181 43974 385 524 145 460 2446 181 1203 408 1031 181 903 181 1601 385 524 145 3025 408 7009 181 2976 385 24237 408 459 181 1478 126 16581 130 24237 408 4403 408 7009 181 49749 388 2444 400 1203 408 7009 181 2976 127 399 883 7082 181 10526 181 9103 181 22440 20978 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 136 26233 207 1177 1127 145 883 7082 181 10526 181 9103 181 16006 26736 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 36066 4728 1177 1127 145 883 7082 181 10526 181 9103 181 7337 1093 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 21034 204 1177 1127 145 883 7082 181 10526 181 9103 181 19872 21034 166 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 207 21034 1177 1127 145 1361 144 2729 181 896 126 16581 130 7082 2\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   idx: 1\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   label: 0\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_int', '_trans', 'code', '(', 'AV', 'Format', 'Context', '_**', 'output', '_', 'files', ',', '_int', '_nb', '_', 'output', '_', 'files', ',', '_Input', 'File', '_*', 'input', '_', 'files', ',', '_int', '_nb', '_', 'input', '_', 'files', ',', '_Stream', 'Map', '_*', 'stream', '_', 'maps', ',', '_int', '_nb', '_', 'stream', '_', 'maps', ')', '_{', '_int', '_ret', '_=', '_0', ',', '_i', ',', '_j', ',', '_k', ',', '_n', ',', '_nb', '_', 'ostream', 's', '_=', '_0', ',', '_step', ';', '_AV', 'Format', 'Context', '_*', 'is', ',', '_*', 'os', ';', '_AV', 'CodecContext', '_*', 'codec', ',', '_*', 'icode', 'c', ';', '_OutputStream', '_*', 'ost', ',', '_**', 'ost', '_', 'table', '_=', '_NULL', ';', '_InputStream', '_*', 'ist', ';', '_char', '_error', '[', '1024', '];', '_int', '_key', ';', '_int', '_want', '_', 'sdp', '_=', '_1', ';', '_uint', '8', '_', 't', '_no', '_', 'packet', '[', 'MAX', '_', 'FILES', ']={', '0', '};', '_int', '_no', '_', 'packet', '_', 'count', '=', '0', ';', '_int', '_nb', '_', 'frame', '_', 'threshold', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_int', '_nb', '_', 'streams', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_if', '_(', 'rate', '_', 'emu', ')', '_for', '_(', 'i', '_=', '_0', ';', '_i', '_<', '_nb', '_', 'input', '_', 'streams', ';', '_i', '++)', '_input', '_', 'streams', '[', 'i', '].', 'start', '_=', '_av', '_', 'gettime', '();', '_/*', '_output', '_stream', '_init', '_*/', '_nb', '_', 'ostream', 's', '_=', '_0', ';', '_for', '(', 'i', '=', '0', ';', 'i', '<', 'nb', '_', 'output', '_', 'files', ';', 'i', '++)', '_{', '_os', '_=', '_output', '_', 'files', '[', 'i', '];', '_if', '_(!', 'os', '->', 'nb', '_', 'streams', '_&&', '_!(', 'os', '->', 'o', 'format', '->', 'flags', '_&', '_A', 'VF', 'MT', '_', 'NO', 'STREAM', 'S', '))', '_{', '_av', '_', 'dump', '_', 'format', '(', 'output', '_', 'files', '[', 'i', '],', '_i', ',', '_output', '_', 'files', '[', 'i', ']->', 'filename', ',', '_1', ');', '_fprintf', '(', 'stderr', ',', '_\"', 'Output', '_file', '_#%', 'd', '_does', '_not', '_contain', '_any', '_stream', '\\\\', 'n', '\",', '_i', ');', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_nb', '_', 'ostream', 's', '_+=', '_os', '->', 'nb', '_', 'streams', ';', '_}', '_if', '_(', 'nb', '_', 'stream', '_', 'maps', '_>', '_0', '_&&', '_nb', '_', 'stream', '_', 'maps', '_!=', '_nb', '_', 'ostream', 's', ')', '_{', '_fprintf', '(', 'stderr', ',', '_\"', 'Number', '_of', '_stream', '_maps', '_must', '_match', '_number', '_of', '_output', '_streams', '\\\\', 'n', '\");', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_/*', '_Sanity', '_check', '_the', '_mapping', '_args', '_--', '_do', '_the', '_input', '_files', '_&', '_streams', '_exist', '?', '</s>']\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   input_ids: 0 932 554 1688 780 126 4543 1660 1133 2760 1788 181 3011 130 554 9091 181 1788 181 3011 130 4715 956 426 1376 181 3011 130 554 9091 181 1376 181 3011 130 7141 1281 426 1719 181 9865 130 554 9091 181 1719 181 9865 127 399 554 1234 385 461 130 548 130 913 130 1085 130 416 130 9091 181 10980 201 385 461 130 4091 145 7082 1660 1133 426 402 130 426 654 145 7082 40294 426 4403 130 426 3888 185 145 33734 426 804 130 2760 804 181 1389 385 1008 145 17932 426 507 145 1108 843 177 8681 823 554 1129 145 554 2947 181 15425 385 524 145 941 142 181 202 1375 181 4017 177 2485 181 16930 29435 134 1441 554 1375 181 4017 181 1048 147 134 145 554 9091 181 1966 181 8595 177 4543 15828 181 1850 181 11963 29435 134 1441 554 9091 181 13250 177 4543 15828 181 1850 181 11963 29435 134 1441 462 400 2779 181 9145 127 563 400 191 385 461 145 548 517 9091 181 1376 181 13250 145 548 1232 1586 181 13250 177 191 859 1094 385 2729 181 45105 523 1067 1721 2239 1796 634 9091 181 10980 201 385 461 145 563 126 191 147 134 145 191 146 4269 181 1788 181 3011 145 191 1232 399 2215 385 1721 181 3011 177 191 823 462 802 654 408 4269 181 13250 698 6943 654 408 197 1478 408 1550 519 553 6644 3563 181 2477 8129 169 509 399 2729 181 4617 181 1478 126 1788 181 3011 177 191 759 548 130 1721 181 3011 177 191 3703 2473 130 524 388 5800 126 4691 130 437 2203 1012 19003 186 2129 800 2910 1800 2239 178 196 475 548 388 1234 385 22451 126 3298 388 4484 2269 145 425 9091 181 10980 201 1054 2215 408 4269 181 13250 145 425 462 400 4269 181 1719 181 9865 711 461 698 9091 181 1719 181 9865 620 9091 181 10980 201 127 399 5800 126 4691 130 437 1934 595 2239 12176 2016 1655 1635 595 1721 16323 178 196 659 1234 385 22451 126 3298 388 4484 2269 145 425 1067 29011 1382 448 4860 1822 2850 1000 448 1586 2966 519 16323 3040 149 2\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   *** Example ***\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   idx: 2\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   label: 0\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_void', '_v', '4', 'l', '2', '_', 'free', '_', 'buffer', '(', 'void', '_*', 'opaque', ',', '_uint', '8', '_', 't', '_*', 'unused', ')', '_{', '_V', '4', 'L', '2', 'Buffer', '*', '_av', 'buf', '_=', '_opaque', ';', '_V', '4', 'L', '2', 'm', '2', 'mContext', '_*', 's', '_=', '_buf', '_', 'to', '_', 'm', '2', 'mctx', '(', 'av', 'buf', ');', '_if', '_(', 'atomic', '_', 'fetch', '_', 'sub', '(&', 'av', 'buf', '->', 'context', '_', 'refcount', ',', '_1', ')', '_==', '_1', ')', '_{', '_atomic', '_', 'fetch', '_', 'sub', '_', 'explicit', '(&', 's', '->', 'refcount', ',', '_1', ',', '_memory', '_', 'order', '_', 'ac', 'q', '_', 'rel', ');', '_if', '_(', 's', '->', 'reinit', ')', '_{', '_if', '_(!', 'atomic', '_', 'load', '(&', 's', '->', 'refcount', '))', '_sem', '_', 'post', '(&', 's', '->', 'ref', 'sync', ');', '_}', '_else', '_if', '_(', 'av', 'buf', '->', 'context', '->', 'stream', 'on', ')', '_ff', '_', 'v', '4', 'l', '2', '_', 'buffer', '_', 'enqueue', '(', 'av', 'buf', ');', '_av', '_', 'buffer', '_', 'unref', '(&', 'av', 'buf', '->', 'context', '_', 'ref', ');', '_}', '_}', '</s>']\n",
      "10/29/2023 21:57:07 - INFO - __main__ -   input_ids: 0 932 723 460 138 194 136 181 1654 181 1601 126 895 426 12458 130 941 142 181 202 426 9983 127 399 1010 138 162 136 1579 128 2729 1091 385 18128 145 1010 138 162 136 195 136 24943 426 201 385 2200 181 589 181 195 136 40828 126 1070 1091 388 462 400 6037 181 5011 181 1175 862 1070 1091 408 1499 181 16660 130 524 127 550 524 127 399 10285 181 5011 181 1175 181 16784 862 201 408 16660 130 524 130 3338 181 1346 181 921 199 181 2526 388 462 400 201 408 41486 127 399 462 802 6037 181 1030 862 201 408 16660 509 10254 181 1892 862 201 408 903 3616 388 425 669 462 400 1070 1091 408 1499 408 1719 382 127 11384 181 204 138 194 136 181 1601 181 13226 126 1070 1091 388 2729 181 1601 181 18327 862 1070 1091 408 1499 181 903 388 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/29/2023 21:57:08 - INFO - __main__ -   ***** Running training *****\n",
      "10/29/2023 21:57:08 - INFO - __main__ -     Num examples = 21854\n",
      "10/29/2023 21:57:08 - INFO - __main__ -     Num Epochs = 3\n",
      "10/29/2023 21:57:08 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/29/2023 21:57:08 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/29/2023 21:57:08 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/29/2023 21:57:08 - INFO - __main__ -     Total optimization steps = 2049\n",
      "  0%|          | 0/683 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.70583:  10%|▉         | 67/683 [01:05<09:05,  1.13it/s] 10/29/2023 21:58:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:58:18 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:58:18 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 21:58:38 - INFO - __main__ -     eval_loss = 0.6832\n",
      "10/29/2023 21:58:38 - INFO - __main__ -     eval_acc = 0.5586\n",
      "10/29/2023 21:58:38 - INFO - __main__ -     eval_f1 = 0.2318\n",
      "10/29/2023 21:58:38 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:58:38 - INFO - __main__ -     Best acc:0.5586\n",
      "10/29/2023 21:58:38 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:58:39 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/29/2023 21:58:39 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:58:39 - INFO - __main__ -     Best f1:0.2318\n",
      "10/29/2023 21:58:39 - INFO - __main__ -     ********************\n",
      "10/29/2023 21:58:40 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.70583:  10%|▉         | 68/683 [01:31<1:29:27,  8.73s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69837:  20%|█▉        | 135/683 [02:31<08:02,  1.14it/s] 10/29/2023 21:59:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 21:59:45 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 21:59:45 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:00:05 - INFO - __main__ -     eval_loss = 0.6818\n",
      "10/29/2023 22:00:05 - INFO - __main__ -     eval_acc = 0.5666\n",
      "10/29/2023 22:00:05 - INFO - __main__ -     eval_f1 = 0.1204\n",
      "10/29/2023 22:00:05 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:00:05 - INFO - __main__ -     Best acc:0.5666\n",
      "10/29/2023 22:00:05 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:00:06 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.69837:  20%|█▉        | 136/683 [02:57<1:18:01,  8.56s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69499:  30%|██▉       | 203/683 [03:57<06:59,  1.14it/s]  10/29/2023 22:01:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 22:01:11 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 22:01:11 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:01:30 - INFO - __main__ -     eval_loss = 0.6817\n",
      "10/29/2023 22:01:30 - INFO - __main__ -     eval_acc = 0.5549\n",
      "10/29/2023 22:01:30 - INFO - __main__ -     eval_f1 = 0.5352\n",
      "10/29/2023 22:01:30 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:01:30 - INFO - __main__ -     Best f1:0.5352\n",
      "10/29/2023 22:01:30 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:01:31 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.69499:  30%|██▉       | 204/683 [04:22<1:07:15,  8.43s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69304:  40%|███▉      | 271/683 [05:23<06:04,  1.13it/s]  10/29/2023 22:02:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 22:02:36 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 22:02:36 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:02:56 - INFO - __main__ -     eval_loss = 0.6846\n",
      "10/29/2023 22:02:56 - INFO - __main__ -     eval_acc = 0.5501\n",
      "10/29/2023 22:02:56 - INFO - __main__ -     eval_f1 = 0.5951\n",
      "10/29/2023 22:02:56 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:02:56 - INFO - __main__ -     Best f1:0.5951\n",
      "10/29/2023 22:02:56 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:02:57 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.69304:  40%|███▉      | 272/683 [05:48<58:42,  8.57s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.69048:  50%|████▉     | 339/683 [06:48<05:00,  1.14it/s]10/29/2023 22:04:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 22:04:02 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 22:04:02 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:04:22 - INFO - __main__ -     eval_loss = 0.6842\n",
      "10/29/2023 22:04:22 - INFO - __main__ -     eval_acc = 0.5626\n",
      "10/29/2023 22:04:22 - INFO - __main__ -     eval_f1 = 0.6023\n",
      "10/29/2023 22:04:22 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:04:22 - INFO - __main__ -     Best f1:0.6023\n",
      "10/29/2023 22:04:22 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:04:22 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.69048:  50%|████▉     | 340/683 [07:14<48:30,  8.49s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.6875:  60%|█████▉    | 407/683 [08:14<04:02,  1.14it/s] 10/29/2023 22:05:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 22:05:27 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 22:05:27 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:05:47 - INFO - __main__ -     eval_loss = 0.6575\n",
      "10/29/2023 22:05:47 - INFO - __main__ -     eval_acc = 0.5787\n",
      "10/29/2023 22:05:47 - INFO - __main__ -     eval_f1 = 0.3588\n",
      "10/29/2023 22:05:47 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:05:47 - INFO - __main__ -     Best acc:0.5787\n",
      "10/29/2023 22:05:47 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:05:47 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.6875:  60%|█████▉    | 408/683 [08:39<38:19,  8.36s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.68298:  70%|██████▉   | 475/683 [09:38<03:01,  1.14it/s]10/29/2023 22:06:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 22:06:52 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 22:06:52 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:07:12 - INFO - __main__ -     eval_loss = 0.6605\n",
      "10/29/2023 22:07:12 - INFO - __main__ -     eval_acc = 0.5886\n",
      "10/29/2023 22:07:12 - INFO - __main__ -     eval_f1 = 0.5405\n",
      "10/29/2023 22:07:12 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:07:12 - INFO - __main__ -     Best acc:0.5886\n",
      "10/29/2023 22:07:12 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:07:13 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.68298:  70%|██████▉   | 476/683 [10:04<29:32,  8.56s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.6788:  80%|███████▉  | 543/683 [11:04<02:01,  1.15it/s] 10/29/2023 22:08:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/29/2023 22:08:17 - INFO - __main__ -     Num examples = 2732\n",
      "10/29/2023 22:08:17 - INFO - __main__ -     Batch size = 64\n",
      "10/29/2023 22:08:37 - INFO - __main__ -     eval_loss = 0.6266\n",
      "10/29/2023 22:08:37 - INFO - __main__ -     eval_acc = 0.6318\n",
      "10/29/2023 22:08:37 - INFO - __main__ -     eval_f1 = 0.5107\n",
      "10/29/2023 22:08:37 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:08:37 - INFO - __main__ -     Best acc:0.6318\n",
      "10/29/2023 22:08:37 - INFO - __main__ -     ********************\n",
      "10/29/2023 22:08:38 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.6788:  80%|███████▉  | 544/683 [11:29<19:55,  8.60s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.67504:  89%|████████▉ | 611/683 [12:30<01:02,  1.14it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 1.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run bce biased first, in train dataset, the divison of negtative/ positive is 10, so pos_weight 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original dataset, study pos_weight 10 vs bias 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 14:13:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 14:13:52 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='BCEWithLogits', BCE_bias=10.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   idx: 0\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   label: 0\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   input_tokens: ['<s>', 'OM', '_', 'uint', '32', '_k', 'g', '_', 'sync', '_', 'cc', 'ache', '_', 'name', '_(', '_krb', '5', '_', 'context', '_context', '_,', '_O', 'M', '_', 'uint', '32', '_*', '_minor', '_', 'status', '_)', '_{', '_O', 'M', '_', 'uint', '32', '_err', '_=', '_0', '_;', '_if', '_(', '_!', '_err', '_)', '_{', '_err', '_=', '_krb', '5', '_', 'cc', '_', 'set', '_', 'default', '_', 'name', '_(', '_context', '_,', '_(', '_char', '_*', '_)', '_k', '5', '_', 'get', 'specific', '_(', '_K', '5', '_', 'KEY', '_', 'GSS', '_', 'KRB', '5', '_', 'CC', 'ACHE', '_', 'NAME', '_)', '_)', '_;', '_}', '_*', '_minor', '_', 'status', '_=', '_err', '_;', '_return', '_(', '_*', '_minor', '_', 'status', '_==', '_0', '_)', '_?', '_GSS', '_', 'S', '_', 'COMPLETE', '_:', '_GSS', '_', 'S', '_', 'FAILURE', '_;', '_}', '</s>']\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   input_ids: 0 1034 181 1446 694 1085 189 181 3616 181 1758 1143 181 616 400 23146 139 181 1499 1552 2019 790 163 181 1446 694 426 12438 181 1366 743 399 790 163 181 1446 694 573 385 461 2476 462 400 552 573 743 399 573 385 23146 139 181 1758 181 491 181 1338 181 616 400 1552 2019 400 1108 426 743 1085 139 181 459 8847 400 2583 139 181 2757 181 37732 181 47593 139 181 2569 21562 181 2524 743 743 2476 425 426 12438 181 1366 385 573 2476 483 400 426 12438 181 1366 550 461 743 999 28753 181 169 181 14818 545 28753 181 169 181 7092 2476 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   idx: 1\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   label: 0\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   input_tokens: ['<s>', 'void', '_K', 'Password', 'Dlg', '::', 'key', 'Pressed', '(', '_Q', 'KeyEvent', '_*', 'e', '_)', '_{', '_static', '_bool', '_waitFor', 'Authentication', '_=', '_false', ';', '_if', '_(!', 'waitFor', 'Authentication', ')', '_{', '_switch', '_(', '_e', '->', 'key', '()', '_)', '_{', '_case', '_Key', '_', 'Back', 'space', ':', '_{', '_int', '_len', '_=', '_password', '.', 'length', '();', '_if', '_(', '_len', '_)', '_{', '_password', '.', 'truncate', '(', '_len', '_-', '_1', '_);', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_}', '_}', '_break', ';', '_case', '_Key', '_', 'Return', ':', '_timer', '.', 'stop', '();', '_waitFor', 'Authentication', '_=', '_true', ';', '_if', '_(', '_try', 'Password', '()', '_)', '_emit', '_pass', 'Ok', '();', '_else', '_{', '_label', '->', 'setText', '(', '_g', 'locale', '->', 'translate', '(\"', 'Failed', '\")', '_);', '_password', '_=', '_\"\";', '_timer', 'Mode', '_=', '_1', ';', '_timer', '.', 'start', '(', '_1500', ',', '_TRUE', '_);', '_}', '_waitFor', 'Authentication', '_=', '_false', ';', '_break', ';', '_case', '_Key', '_', 'Escape', ':', '_emit', '_pass', 'Cancel', '();', '_break', ';', '_default', ':', '_if', '_(', '_password', '.', 'length', '()', '_<', '_MAX', '_', 'PASSWORD', '_', 'LENGTH', '_)', '_{', '_password', '_+=', '_(', 'char', ')', 'e', '->', 'ascii', '();', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_timer', '.', 'change', 'Interval', '(', '_10000', '_);', '_}', '_}', '_}', '_}', '</s>']\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   input_ids: 0 895 2583 5256 7560 500 671 12528 126 1152 21803 426 187 743 399 1176 1223 20564 9832 385 766 145 462 802 14457 9832 127 399 2444 400 572 408 671 429 743 399 883 4384 181 2432 1428 144 399 554 1015 385 5724 132 977 523 462 400 1015 743 399 5724 132 13327 126 1015 581 524 857 462 126 558 16468 743 3924 10621 201 523 425 425 1127 145 883 4384 181 1675 144 5912 132 3156 523 20564 9832 385 769 145 462 400 1568 5256 429 743 5311 3198 6358 523 669 399 2649 408 8071 126 611 4727 408 7348 503 3624 807 857 5724 385 4986 5912 1649 385 524 145 5912 132 1094 126 35639 130 3876 857 425 20564 9832 385 766 145 1127 145 883 4384 181 9586 144 5311 3198 5686 523 1127 145 1361 144 462 400 5724 132 977 429 517 4789 181 20016 181 6905 743 399 5724 1054 400 1285 127 187 408 9654 523 462 126 558 16468 743 3924 10621 201 523 5912 132 2999 4655 126 12230 857 425 425 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   idx: 2\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   label: 0\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_int', '_trans', 'code', '(', 'AV', 'Format', 'Context', '_**', 'output', '_', 'files', ',', '_int', '_nb', '_', 'output', '_', 'files', ',', '_Input', 'File', '_*', 'input', '_', 'files', ',', '_int', '_nb', '_', 'input', '_', 'files', ',', '_Stream', 'Map', '_*', 'stream', '_', 'maps', ',', '_int', '_nb', '_', 'stream', '_', 'maps', ')', '_{', '_int', '_ret', '_=', '_0', ',', '_i', ',', '_j', ',', '_k', ',', '_n', ',', '_nb', '_', 'ostream', 's', '_=', '_0', ',', '_step', ';', '_AV', 'Format', 'Context', '_*', 'is', ',', '_*', 'os', ';', '_AV', 'CodecContext', '_*', 'codec', ',', '_*', 'icode', 'c', ';', '_OutputStream', '_*', 'ost', ',', '_**', 'ost', '_', 'table', '_=', '_NULL', ';', '_InputStream', '_*', 'ist', ';', '_char', '_error', '[', '1024', '];', '_int', '_key', ';', '_int', '_want', '_', 'sdp', '_=', '_1', ';', '_uint', '8', '_', 't', '_no', '_', 'packet', '[', 'MAX', '_', 'FILES', ']={', '0', '};', '_int', '_no', '_', 'packet', '_', 'count', '=', '0', ';', '_int', '_nb', '_', 'frame', '_', 'threshold', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_int', '_nb', '_', 'streams', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_if', '_(', 'rate', '_', 'emu', ')', '_for', '_(', 'i', '_=', '_0', ';', '_i', '_<', '_nb', '_', 'input', '_', 'streams', ';', '_i', '++)', '_input', '_', 'streams', '[', 'i', '].', 'start', '_=', '_av', '_', 'gettime', '();', '_/*', '_output', '_stream', '_init', '_*/', '_nb', '_', 'ostream', 's', '_=', '_0', ';', '_for', '(', 'i', '=', '0', ';', 'i', '<', 'nb', '_', 'output', '_', 'files', ';', 'i', '++)', '_{', '_os', '_=', '_output', '_', 'files', '[', 'i', '];', '_if', '_(!', 'os', '->', 'nb', '_', 'streams', '_&&', '_!(', 'os', '->', 'o', 'format', '->', 'flags', '_&', '_A', 'VF', 'MT', '_', 'NO', 'STREAM', 'S', '))', '_{', '_av', '_', 'dump', '_', 'format', '(', 'output', '_', 'files', '[', 'i', '],', '_i', ',', '_output', '_', 'files', '[', 'i', ']->', 'filename', ',', '_1', ');', '_fprintf', '(', 'stderr', ',', '_\"', 'Output', '_file', '_#%', 'd', '_does', '_not', '_contain', '_any', '_stream', '\\\\', 'n', '\",', '_i', ');', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_nb', '_', 'ostream', 's', '_+=', '_os', '->', 'nb', '_', 'streams', ';', '_}', '_if', '_(', 'nb', '_', 'stream', '_', 'maps', '_>', '_0', '_&&', '_nb', '_', 'stream', '_', 'maps', '_!=', '_nb', '_', 'ostream', 's', ')', '_{', '_fprintf', '(', 'stderr', ',', '_\"', 'Number', '_of', '_stream', '_maps', '_must', '_match', '_number', '_of', '_output', '_streams', '\\\\', 'n', '\");', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_/*', '_Sanity', '_check', '_the', '_mapping', '_args', '_--', '_do', '_the', '_input', '_files', '_&', '_streams', '_exist', '?', '</s>']\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   input_ids: 0 932 554 1688 780 126 4543 1660 1133 2760 1788 181 3011 130 554 9091 181 1788 181 3011 130 4715 956 426 1376 181 3011 130 554 9091 181 1376 181 3011 130 7141 1281 426 1719 181 9865 130 554 9091 181 1719 181 9865 127 399 554 1234 385 461 130 548 130 913 130 1085 130 416 130 9091 181 10980 201 385 461 130 4091 145 7082 1660 1133 426 402 130 426 654 145 7082 40294 426 4403 130 426 3888 185 145 33734 426 804 130 2760 804 181 1389 385 1008 145 17932 426 507 145 1108 843 177 8681 823 554 1129 145 554 2947 181 15425 385 524 145 941 142 181 202 1375 181 4017 177 2485 181 16930 29435 134 1441 554 1375 181 4017 181 1048 147 134 145 554 9091 181 1966 181 8595 177 4543 15828 181 1850 181 11963 29435 134 1441 554 9091 181 13250 177 4543 15828 181 1850 181 11963 29435 134 1441 462 400 2779 181 9145 127 563 400 191 385 461 145 548 517 9091 181 1376 181 13250 145 548 1232 1586 181 13250 177 191 859 1094 385 2729 181 45105 523 1067 1721 2239 1796 634 9091 181 10980 201 385 461 145 563 126 191 147 134 145 191 146 4269 181 1788 181 3011 145 191 1232 399 2215 385 1721 181 3011 177 191 823 462 802 654 408 4269 181 13250 698 6943 654 408 197 1478 408 1550 519 553 6644 3563 181 2477 8129 169 509 399 2729 181 4617 181 1478 126 1788 181 3011 177 191 759 548 130 1721 181 3011 177 191 3703 2473 130 524 388 5800 126 4691 130 437 2203 1012 19003 186 2129 800 2910 1800 2239 178 196 475 548 388 1234 385 22451 126 3298 388 4484 2269 145 425 9091 181 10980 201 1054 2215 408 4269 181 13250 145 425 462 400 4269 181 1719 181 9865 711 461 698 9091 181 1719 181 9865 620 9091 181 10980 201 127 399 5800 126 4691 130 437 1934 595 2239 12176 2016 1655 1635 595 1721 16323 178 196 659 1234 385 22451 126 3298 388 4484 2269 145 425 1067 29011 1382 448 4860 1822 2850 1000 448 1586 2966 519 16323 3040 149 2\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/30/2023 14:21:06 - INFO - __main__ -   ***** Running training *****\n",
      "10/30/2023 14:21:06 - INFO - __main__ -     Num examples = 302111\n",
      "10/30/2023 14:21:06 - INFO - __main__ -     Num Epochs = 1\n",
      "10/30/2023 14:21:06 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/30/2023 14:21:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/30/2023 14:21:06 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/30/2023 14:21:06 - INFO - __main__ -     Total optimization steps = 9441\n",
      "  0%|          | 0/9441 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 1.10178:  10%|▉         | 943/9441 [14:40<2:02:55,  1.15it/s]10/30/2023 14:36:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 14:36:38 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 14:36:38 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 14:41:33 - INFO - __main__ -     eval_loss = 1.0075\n",
      "10/30/2023 14:41:33 - INFO - __main__ -     eval_acc = 0.6368\n",
      "10/30/2023 14:41:33 - INFO - __main__ -     eval_f1 = 0.2598\n",
      "10/30/2023 14:41:33 - INFO - __main__ -     ********************\n",
      "10/30/2023 14:41:33 - INFO - __main__ -     Best acc:0.6368\n",
      "10/30/2023 14:41:33 - INFO - __main__ -     ********************\n",
      "10/30/2023 14:41:34 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 14:41:34 - INFO - __main__ -     ********************\n",
      "10/30/2023 14:41:34 - INFO - __main__ -     Best f1:0.2598\n",
      "10/30/2023 14:41:34 - INFO - __main__ -     ********************\n",
      "10/30/2023 14:41:34 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 1.10178:  10%|▉         | 944/9441 [20:28<248:20:58, 105.22s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 1.07797:  20%|█▉        | 1887/9441 [35:01<1:47:53,  1.17it/s]  10/30/2023 14:57:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 14:57:00 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 14:57:00 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 15:01:48 - INFO - __main__ -     eval_loss = 0.9957\n",
      "10/30/2023 15:01:48 - INFO - __main__ -     eval_acc = 0.6459\n",
      "10/30/2023 15:01:48 - INFO - __main__ -     eval_f1 = 0.2698\n",
      "10/30/2023 15:01:48 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:01:48 - INFO - __main__ -     Best acc:0.6459\n",
      "10/30/2023 15:01:48 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:01:49 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 15:01:49 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:01:49 - INFO - __main__ -     Best f1:0.2698\n",
      "10/30/2023 15:01:49 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:01:49 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 1.07797:  20%|█▉        | 1888/9441 [40:42<216:59:42, 103.43s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 1.05018:  30%|██▉       | 2831/9441 [55:16<1:34:47,  1.16it/s]   10/30/2023 15:17:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 15:17:16 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 15:17:16 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 15:22:08 - INFO - __main__ -     eval_loss = 0.9401\n",
      "10/30/2023 15:22:08 - INFO - __main__ -     eval_acc = 0.7401\n",
      "10/30/2023 15:22:08 - INFO - __main__ -     eval_f1 = 0.3174\n",
      "10/30/2023 15:22:08 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:22:08 - INFO - __main__ -     Best acc:0.7401\n",
      "10/30/2023 15:22:08 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:22:09 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 15:22:09 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:22:09 - INFO - __main__ -     Best f1:0.3174\n",
      "10/30/2023 15:22:09 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:22:09 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 1.05018:  30%|██▉       | 2832/9441 [1:01:02<192:30:20, 104.86s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 1.02867:  40%|███▉      | 3775/9441 [1:15:42<1:23:12,  1.14it/s]   10/30/2023 15:37:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 15:37:39 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 15:37:39 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 15:42:37 - INFO - __main__ -     eval_loss = 1.0015\n",
      "10/30/2023 15:42:37 - INFO - __main__ -     eval_acc = 0.8255\n",
      "10/30/2023 15:42:37 - INFO - __main__ -     eval_f1 = 0.3693\n",
      "10/30/2023 15:42:37 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:42:37 - INFO - __main__ -     Best acc:0.8255\n",
      "10/30/2023 15:42:37 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:42:38 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 15:42:38 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:42:38 - INFO - __main__ -     Best f1:0.3693\n",
      "10/30/2023 15:42:38 - INFO - __main__ -     ********************\n",
      "10/30/2023 15:42:39 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 1.02867:  40%|███▉      | 3776/9441 [1:21:32<166:30:58, 105.82s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 1.01529:  50%|████▉     | 4719/9441 [1:36:02<1:07:55,  1.16it/s]   10/30/2023 15:58:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 15:58:02 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 15:58:02 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 16:02:48 - INFO - __main__ -     eval_loss = 0.8948\n",
      "10/30/2023 16:02:48 - INFO - __main__ -     eval_acc = 0.7865\n",
      "10/30/2023 16:02:48 - INFO - __main__ -     eval_f1 = 0.3555\n",
      "epoch 0 loss 1.01529:  50%|████▉     | 4720/9441 [1:41:41<134:25:45, 102.51s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 1.00563:  60%|█████▉    | 5663/9441 [1:56:14<1:12:53,  1.16s/it]   10/30/2023 16:18:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 16:18:11 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 16:18:11 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 16:23:03 - INFO - __main__ -     eval_loss = 0.9378\n",
      "10/30/2023 16:23:03 - INFO - __main__ -     eval_acc = 0.8148\n",
      "10/30/2023 16:23:03 - INFO - __main__ -     eval_f1 = 0.3716\n",
      "10/30/2023 16:23:03 - INFO - __main__ -     ********************\n",
      "10/30/2023 16:23:03 - INFO - __main__ -     Best f1:0.3716\n",
      "10/30/2023 16:23:03 - INFO - __main__ -     ********************\n",
      "10/30/2023 16:23:04 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 1.00563:  60%|█████▉    | 5664/9441 [2:01:57<109:13:01, 104.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.99723:  70%|██████▉   | 6607/9441 [2:16:30<40:27,  1.17it/s]     10/30/2023 16:38:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 16:38:29 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 16:38:29 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 16:43:23 - INFO - __main__ -     eval_loss = 0.8875\n",
      "10/30/2023 16:43:23 - INFO - __main__ -     eval_acc = 0.8087\n",
      "10/30/2023 16:43:23 - INFO - __main__ -     eval_f1 = 0.3702\n",
      "epoch 0 loss 0.99723:  70%|██████▉   | 6608/9441 [2:22:16<82:24:41, 104.72s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.9893:  80%|███████▉  | 7551/9441 [2:36:50<27:23,  1.15it/s]     10/30/2023 16:58:46 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 16:58:46 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 16:58:46 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 17:03:42 - INFO - __main__ -     eval_loss = 0.9386\n",
      "10/30/2023 17:03:42 - INFO - __main__ -     eval_acc = 0.8498\n",
      "10/30/2023 17:03:42 - INFO - __main__ -     eval_f1 = 0.3975\n",
      "10/30/2023 17:03:42 - INFO - __main__ -     ********************\n",
      "10/30/2023 17:03:42 - INFO - __main__ -     Best acc:0.8498\n",
      "10/30/2023 17:03:42 - INFO - __main__ -     ********************\n",
      "10/30/2023 17:03:43 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 17:03:43 - INFO - __main__ -     ********************\n",
      "10/30/2023 17:03:43 - INFO - __main__ -     Best f1:0.3975\n",
      "10/30/2023 17:03:43 - INFO - __main__ -     ********************\n",
      "10/30/2023 17:03:44 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.9893:  80%|███████▉  | 7552/9441 [2:42:37<55:10:16, 105.14s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.97969:  90%|████████▉ | 8495/9441 [2:57:12<14:21,  1.10it/s]    10/30/2023 17:19:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 17:19:09 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 17:19:09 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 17:24:02 - INFO - __main__ -     eval_loss = 0.8914\n",
      "10/30/2023 17:24:02 - INFO - __main__ -     eval_acc = 0.8188\n",
      "10/30/2023 17:24:02 - INFO - __main__ -     eval_f1 = 0.383\n",
      "epoch 0 loss 0.97969:  90%|████████▉ | 8496/9441 [3:02:55<27:14:29, 103.78s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.96975: 100%|█████████▉| 9439/9441 [3:17:25<00:01,  1.16it/s]    10/30/2023 17:39:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 17:39:26 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 17:39:26 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 17:44:18 - INFO - __main__ -     eval_loss = 0.8899\n",
      "10/30/2023 17:44:18 - INFO - __main__ -     eval_acc = 0.8136\n",
      "10/30/2023 17:44:18 - INFO - __main__ -     eval_f1 = 0.3783\n",
      "epoch 0 loss 0.96975: 100%|█████████▉| 9440/9441 [3:23:11<01:44, 104.63s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.96975: 100%|██████████| 9441/9441 [3:23:12<00:00,  1.29s/it] \n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 1 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 10.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pos_weight 10 model, best acc check point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 18:10:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 18:10:58 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='BCEWithLogits', BCE_bias=10.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 18:11:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 18:11:57 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 18:11:57 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/30/2023 18:16:37 - INFO - __main__ -   ***** Eval results *****\n",
      "10/30/2023 18:16:37 - INFO - __main__ -     eval_acc = 0.8498\n",
      "10/30/2023 18:16:37 - INFO - __main__ -     eval_f1 = 0.3975\n",
      "10/30/2023 18:16:37 - INFO - __main__ -     eval_loss = 0.9386\n",
      "10/30/2023 18:17:32 - INFO - __main__ -   ***** Running Test *****\n",
      "10/30/2023 18:17:32 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 18:17:32 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 591/591 [04:31<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 10.0\\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.8450599793448613}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study pos_weight 10, best f1 checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 18:23:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 18:23:44 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='BCEWithLogits', BCE_bias=10.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 18:24:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 18:24:43 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 18:24:43 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/30/2023 18:29:21 - INFO - __main__ -   ***** Eval results *****\n",
      "10/30/2023 18:29:21 - INFO - __main__ -     eval_acc = 0.8498\n",
      "10/30/2023 18:29:21 - INFO - __main__ -     eval_f1 = 0.3975\n",
      "10/30/2023 18:29:21 - INFO - __main__ -     eval_loss = 0.9386\n",
      "10/30/2023 18:30:17 - INFO - __main__ -   ***** Running Test *****\n",
      "10/30/2023 18:30:17 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 18:30:17 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 591/591 [04:29<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 10.0\\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.8450599793448613}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For above, it's not fully trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study weight 10, acc based model!  WE USE WEIGHT NOT pos weight\n",
    "# f1 is higher compared to normal loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 18:10:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 10.0\\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accidentally restart the above func, the f1 of that model is 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.9185975690490692}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study weight 10, f1 based model WE USE WEIGHT NOT pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 13:18:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 13:18:55 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='BCEWithLogits', BCE_bias=10.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 13:19:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 13:19:53 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 13:19:53 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/30/2023 13:24:26 - INFO - __main__ -   ***** Eval results *****\n",
      "10/30/2023 13:24:26 - INFO - __main__ -     eval_acc = 0.9138\n",
      "10/30/2023 13:24:26 - INFO - __main__ -     eval_f1 = 0.3649\n",
      "10/30/2023 13:24:26 - INFO - __main__ -     eval_loss = 2.3383\n",
      "10/30/2023 13:25:21 - INFO - __main__ -   ***** Running Test *****\n",
      "10/30/2023 13:25:21 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 13:25:21 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 591/591 [04:27<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 10.0\\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.9110769801128088}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 08:17:57 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 08:18:00 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   idx: 0\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   label: 0\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   input_tokens: ['<s>', 'OM', '_', 'uint', '32', '_k', 'g', '_', 'sync', '_', 'cc', 'ache', '_', 'name', '_(', '_krb', '5', '_', 'context', '_context', '_,', '_O', 'M', '_', 'uint', '32', '_*', '_minor', '_', 'status', '_)', '_{', '_O', 'M', '_', 'uint', '32', '_err', '_=', '_0', '_;', '_if', '_(', '_!', '_err', '_)', '_{', '_err', '_=', '_krb', '5', '_', 'cc', '_', 'set', '_', 'default', '_', 'name', '_(', '_context', '_,', '_(', '_char', '_*', '_)', '_k', '5', '_', 'get', 'specific', '_(', '_K', '5', '_', 'KEY', '_', 'GSS', '_', 'KRB', '5', '_', 'CC', 'ACHE', '_', 'NAME', '_)', '_)', '_;', '_}', '_*', '_minor', '_', 'status', '_=', '_err', '_;', '_return', '_(', '_*', '_minor', '_', 'status', '_==', '_0', '_)', '_?', '_GSS', '_', 'S', '_', 'COMPLETE', '_:', '_GSS', '_', 'S', '_', 'FAILURE', '_;', '_}', '</s>']\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   input_ids: 0 1034 181 1446 694 1085 189 181 3616 181 1758 1143 181 616 400 23146 139 181 1499 1552 2019 790 163 181 1446 694 426 12438 181 1366 743 399 790 163 181 1446 694 573 385 461 2476 462 400 552 573 743 399 573 385 23146 139 181 1758 181 491 181 1338 181 616 400 1552 2019 400 1108 426 743 1085 139 181 459 8847 400 2583 139 181 2757 181 37732 181 47593 139 181 2569 21562 181 2524 743 743 2476 425 426 12438 181 1366 385 573 2476 483 400 426 12438 181 1366 550 461 743 999 28753 181 169 181 14818 545 28753 181 169 181 7092 2476 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   idx: 1\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   label: 0\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   input_tokens: ['<s>', 'void', '_K', 'Password', 'Dlg', '::', 'key', 'Pressed', '(', '_Q', 'KeyEvent', '_*', 'e', '_)', '_{', '_static', '_bool', '_waitFor', 'Authentication', '_=', '_false', ';', '_if', '_(!', 'waitFor', 'Authentication', ')', '_{', '_switch', '_(', '_e', '->', 'key', '()', '_)', '_{', '_case', '_Key', '_', 'Back', 'space', ':', '_{', '_int', '_len', '_=', '_password', '.', 'length', '();', '_if', '_(', '_len', '_)', '_{', '_password', '.', 'truncate', '(', '_len', '_-', '_1', '_);', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_}', '_}', '_break', ';', '_case', '_Key', '_', 'Return', ':', '_timer', '.', 'stop', '();', '_waitFor', 'Authentication', '_=', '_true', ';', '_if', '_(', '_try', 'Password', '()', '_)', '_emit', '_pass', 'Ok', '();', '_else', '_{', '_label', '->', 'setText', '(', '_g', 'locale', '->', 'translate', '(\"', 'Failed', '\")', '_);', '_password', '_=', '_\"\";', '_timer', 'Mode', '_=', '_1', ';', '_timer', '.', 'start', '(', '_1500', ',', '_TRUE', '_);', '_}', '_waitFor', 'Authentication', '_=', '_false', ';', '_break', ';', '_case', '_Key', '_', 'Escape', ':', '_emit', '_pass', 'Cancel', '();', '_break', ';', '_default', ':', '_if', '_(', '_password', '.', 'length', '()', '_<', '_MAX', '_', 'PASSWORD', '_', 'LENGTH', '_)', '_{', '_password', '_+=', '_(', 'char', ')', 'e', '->', 'ascii', '();', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_timer', '.', 'change', 'Interval', '(', '_10000', '_);', '_}', '_}', '_}', '_}', '</s>']\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   input_ids: 0 895 2583 5256 7560 500 671 12528 126 1152 21803 426 187 743 399 1176 1223 20564 9832 385 766 145 462 802 14457 9832 127 399 2444 400 572 408 671 429 743 399 883 4384 181 2432 1428 144 399 554 1015 385 5724 132 977 523 462 400 1015 743 399 5724 132 13327 126 1015 581 524 857 462 126 558 16468 743 3924 10621 201 523 425 425 1127 145 883 4384 181 1675 144 5912 132 3156 523 20564 9832 385 769 145 462 400 1568 5256 429 743 5311 3198 6358 523 669 399 2649 408 8071 126 611 4727 408 7348 503 3624 807 857 5724 385 4986 5912 1649 385 524 145 5912 132 1094 126 35639 130 3876 857 425 20564 9832 385 766 145 1127 145 883 4384 181 9586 144 5311 3198 5686 523 1127 145 1361 144 462 400 5724 132 977 429 517 4789 181 20016 181 6905 743 399 5724 1054 400 1285 127 187 408 9654 523 462 126 558 16468 743 3924 10621 201 523 5912 132 2999 4655 126 12230 857 425 425 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   idx: 2\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   label: 0\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_int', '_trans', 'code', '(', 'AV', 'Format', 'Context', '_**', 'output', '_', 'files', ',', '_int', '_nb', '_', 'output', '_', 'files', ',', '_Input', 'File', '_*', 'input', '_', 'files', ',', '_int', '_nb', '_', 'input', '_', 'files', ',', '_Stream', 'Map', '_*', 'stream', '_', 'maps', ',', '_int', '_nb', '_', 'stream', '_', 'maps', ')', '_{', '_int', '_ret', '_=', '_0', ',', '_i', ',', '_j', ',', '_k', ',', '_n', ',', '_nb', '_', 'ostream', 's', '_=', '_0', ',', '_step', ';', '_AV', 'Format', 'Context', '_*', 'is', ',', '_*', 'os', ';', '_AV', 'CodecContext', '_*', 'codec', ',', '_*', 'icode', 'c', ';', '_OutputStream', '_*', 'ost', ',', '_**', 'ost', '_', 'table', '_=', '_NULL', ';', '_InputStream', '_*', 'ist', ';', '_char', '_error', '[', '1024', '];', '_int', '_key', ';', '_int', '_want', '_', 'sdp', '_=', '_1', ';', '_uint', '8', '_', 't', '_no', '_', 'packet', '[', 'MAX', '_', 'FILES', ']={', '0', '};', '_int', '_no', '_', 'packet', '_', 'count', '=', '0', ';', '_int', '_nb', '_', 'frame', '_', 'threshold', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_int', '_nb', '_', 'streams', '[', 'AV', 'MEDIA', '_', 'TYPE', '_', 'NB', ']={', '0', '};', '_if', '_(', 'rate', '_', 'emu', ')', '_for', '_(', 'i', '_=', '_0', ';', '_i', '_<', '_nb', '_', 'input', '_', 'streams', ';', '_i', '++)', '_input', '_', 'streams', '[', 'i', '].', 'start', '_=', '_av', '_', 'gettime', '();', '_/*', '_output', '_stream', '_init', '_*/', '_nb', '_', 'ostream', 's', '_=', '_0', ';', '_for', '(', 'i', '=', '0', ';', 'i', '<', 'nb', '_', 'output', '_', 'files', ';', 'i', '++)', '_{', '_os', '_=', '_output', '_', 'files', '[', 'i', '];', '_if', '_(!', 'os', '->', 'nb', '_', 'streams', '_&&', '_!(', 'os', '->', 'o', 'format', '->', 'flags', '_&', '_A', 'VF', 'MT', '_', 'NO', 'STREAM', 'S', '))', '_{', '_av', '_', 'dump', '_', 'format', '(', 'output', '_', 'files', '[', 'i', '],', '_i', ',', '_output', '_', 'files', '[', 'i', ']->', 'filename', ',', '_1', ');', '_fprintf', '(', 'stderr', ',', '_\"', 'Output', '_file', '_#%', 'd', '_does', '_not', '_contain', '_any', '_stream', '\\\\', 'n', '\",', '_i', ');', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_nb', '_', 'ostream', 's', '_+=', '_os', '->', 'nb', '_', 'streams', ';', '_}', '_if', '_(', 'nb', '_', 'stream', '_', 'maps', '_>', '_0', '_&&', '_nb', '_', 'stream', '_', 'maps', '_!=', '_nb', '_', 'ostream', 's', ')', '_{', '_fprintf', '(', 'stderr', ',', '_\"', 'Number', '_of', '_stream', '_maps', '_must', '_match', '_number', '_of', '_output', '_streams', '\\\\', 'n', '\");', '_ret', '_=', '_AVERROR', '(', 'EINVAL', ');', '_goto', '_fail', ';', '_}', '_/*', '_Sanity', '_check', '_the', '_mapping', '_args', '_--', '_do', '_the', '_input', '_files', '_&', '_streams', '_exist', '?', '</s>']\n",
      "10/30/2023 08:25:09 - INFO - __main__ -   input_ids: 0 932 554 1688 780 126 4543 1660 1133 2760 1788 181 3011 130 554 9091 181 1788 181 3011 130 4715 956 426 1376 181 3011 130 554 9091 181 1376 181 3011 130 7141 1281 426 1719 181 9865 130 554 9091 181 1719 181 9865 127 399 554 1234 385 461 130 548 130 913 130 1085 130 416 130 9091 181 10980 201 385 461 130 4091 145 7082 1660 1133 426 402 130 426 654 145 7082 40294 426 4403 130 426 3888 185 145 33734 426 804 130 2760 804 181 1389 385 1008 145 17932 426 507 145 1108 843 177 8681 823 554 1129 145 554 2947 181 15425 385 524 145 941 142 181 202 1375 181 4017 177 2485 181 16930 29435 134 1441 554 1375 181 4017 181 1048 147 134 145 554 9091 181 1966 181 8595 177 4543 15828 181 1850 181 11963 29435 134 1441 554 9091 181 13250 177 4543 15828 181 1850 181 11963 29435 134 1441 462 400 2779 181 9145 127 563 400 191 385 461 145 548 517 9091 181 1376 181 13250 145 548 1232 1586 181 13250 177 191 859 1094 385 2729 181 45105 523 1067 1721 2239 1796 634 9091 181 10980 201 385 461 145 563 126 191 147 134 145 191 146 4269 181 1788 181 3011 145 191 1232 399 2215 385 1721 181 3011 177 191 823 462 802 654 408 4269 181 13250 698 6943 654 408 197 1478 408 1550 519 553 6644 3563 181 2477 8129 169 509 399 2729 181 4617 181 1478 126 1788 181 3011 177 191 759 548 130 1721 181 3011 177 191 3703 2473 130 524 388 5800 126 4691 130 437 2203 1012 19003 186 2129 800 2910 1800 2239 178 196 475 548 388 1234 385 22451 126 3298 388 4484 2269 145 425 9091 181 10980 201 1054 2215 408 4269 181 13250 145 425 462 400 4269 181 1719 181 9865 711 461 698 9091 181 1719 181 9865 620 9091 181 10980 201 127 399 5800 126 4691 130 437 1934 595 2239 12176 2016 1655 1635 595 1721 16323 178 196 659 1234 385 22451 126 3298 388 4484 2269 145 425 1067 29011 1382 448 4860 1822 2850 1000 448 1586 2966 519 16323 3040 149 2\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/30/2023 08:25:10 - INFO - __main__ -   ***** Running training *****\n",
      "10/30/2023 08:25:10 - INFO - __main__ -     Num examples = 302111\n",
      "10/30/2023 08:25:10 - INFO - __main__ -     Num Epochs = 3\n",
      "10/30/2023 08:25:10 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/30/2023 08:25:10 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/30/2023 08:25:10 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/30/2023 08:25:10 - INFO - __main__ -     Total optimization steps = 28323\n",
      "  0%|          | 0/9441 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.30423:  10%|▉         | 943/9441 [14:37<2:01:38,  1.16it/s]10/30/2023 08:40:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 08:40:38 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 08:40:38 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 08:45:30 - INFO - __main__ -     eval_loss = 0.2612\n",
      "10/30/2023 08:45:30 - INFO - __main__ -     eval_acc = 0.9174\n",
      "10/30/2023 08:45:30 - INFO - __main__ -     eval_f1 = 0.0\n",
      "10/30/2023 08:45:30 - INFO - __main__ -     ********************\n",
      "10/30/2023 08:45:30 - INFO - __main__ -     Best acc:0.9174\n",
      "10/30/2023 08:45:30 - INFO - __main__ -     ********************\n",
      "10/30/2023 08:45:31 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.30423:  10%|▉         | 944/9441 [20:21<245:34:21, 104.04s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.28378:  20%|█▉        | 1887/9441 [34:49<1:47:58,  1.17it/s]  10/30/2023 09:00:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 09:00:52 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 09:00:52 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 09:05:46 - INFO - __main__ -     eval_loss = 0.2472\n",
      "10/30/2023 09:05:46 - INFO - __main__ -     eval_acc = 0.9172\n",
      "10/30/2023 09:05:46 - INFO - __main__ -     eval_f1 = 0.0089\n",
      "10/30/2023 09:05:46 - INFO - __main__ -     ********************\n",
      "10/30/2023 09:05:46 - INFO - __main__ -     Best f1:0.0089\n",
      "10/30/2023 09:05:46 - INFO - __main__ -     ********************\n",
      "10/30/2023 09:05:47 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.28378:  20%|█▉        | 1888/9441 [40:37<220:25:02, 105.06s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.27344:  30%|██▉       | 2831/9441 [55:08<1:36:25,  1.14it/s]   10/30/2023 09:21:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 09:21:09 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 09:21:09 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 09:26:04 - INFO - __main__ -     eval_loss = 0.242\n",
      "10/30/2023 09:26:04 - INFO - __main__ -     eval_acc = 0.913\n",
      "10/30/2023 09:26:04 - INFO - __main__ -     eval_f1 = 0.1475\n",
      "10/30/2023 09:26:04 - INFO - __main__ -     ********************\n",
      "10/30/2023 09:26:04 - INFO - __main__ -     Best f1:0.1475\n",
      "10/30/2023 09:26:04 - INFO - __main__ -     ********************\n",
      "10/30/2023 09:26:04 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.27344:  30%|██▉       | 2832/9441 [1:00:54<192:10:05, 104.68s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26559:  40%|███▉      | 3775/9441 [1:15:25<1:21:24,  1.16it/s]   10/30/2023 09:41:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 09:41:27 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 09:41:27 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 09:46:19 - INFO - __main__ -     eval_loss = 0.2436\n",
      "10/30/2023 09:46:19 - INFO - __main__ -     eval_acc = 0.9174\n",
      "10/30/2023 09:46:19 - INFO - __main__ -     eval_f1 = 0.0006\n",
      "epoch 0 loss 0.26559:  40%|███▉      | 3776/9441 [1:21:09<163:53:05, 104.15s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26026:  50%|████▉     | 4719/9441 [1:35:37<1:07:33,  1.16it/s]   10/30/2023 10:01:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 10:01:40 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 10:01:40 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 10:06:34 - INFO - __main__ -     eval_loss = 0.2311\n",
      "10/30/2023 10:06:34 - INFO - __main__ -     eval_acc = 0.9178\n",
      "10/30/2023 10:06:34 - INFO - __main__ -     eval_f1 = 0.0152\n",
      "10/30/2023 10:06:34 - INFO - __main__ -     ********************\n",
      "10/30/2023 10:06:34 - INFO - __main__ -     Best acc:0.9178\n",
      "10/30/2023 10:06:34 - INFO - __main__ -     ********************\n",
      "10/30/2023 10:06:35 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.26026:  50%|████▉     | 4720/9441 [1:41:25<137:52:05, 105.13s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.25696:  60%|█████▉    | 5663/9441 [1:55:54<54:03,  1.16it/s]     10/30/2023 10:21:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 10:21:57 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 10:21:57 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 10:26:46 - INFO - __main__ -     eval_loss = 0.2357\n",
      "10/30/2023 10:26:46 - INFO - __main__ -     eval_acc = 0.9177\n",
      "10/30/2023 10:26:46 - INFO - __main__ -     eval_f1 = 0.0152\n",
      "epoch 0 loss 0.25696:  60%|█████▉    | 5664/9441 [2:01:36<108:30:06, 103.42s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.25414:  70%|██████▉   | 6607/9441 [2:16:08<53:01,  1.12s/it]     10/30/2023 10:42:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 10:42:08 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 10:42:08 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 10:46:56 - INFO - __main__ -     eval_loss = 0.2252\n",
      "10/30/2023 10:46:56 - INFO - __main__ -     eval_acc = 0.9194\n",
      "10/30/2023 10:46:56 - INFO - __main__ -     eval_f1 = 0.0913\n",
      "10/30/2023 10:46:56 - INFO - __main__ -     ********************\n",
      "10/30/2023 10:46:56 - INFO - __main__ -     Best acc:0.9194\n",
      "10/30/2023 10:46:56 - INFO - __main__ -     ********************\n",
      "10/30/2023 10:46:57 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.25414:  70%|██████▉   | 6608/9441 [2:21:47<80:55:57, 102.84s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.25154:  80%|███████▉  | 7551/9441 [2:36:18<27:07,  1.16it/s]    10/30/2023 11:02:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 11:02:21 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 11:02:21 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 11:07:13 - INFO - __main__ -     eval_loss = 0.2433\n",
      "10/30/2023 11:07:13 - INFO - __main__ -     eval_acc = 0.919\n",
      "10/30/2023 11:07:13 - INFO - __main__ -     eval_f1 = 0.0866\n",
      "epoch 0 loss 0.25154:  80%|███████▉  | 7552/9441 [2:42:02<54:38:26, 104.13s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.24871:  90%|████████▉ | 8495/9441 [2:56:32<13:28,  1.17it/s]    10/30/2023 11:22:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 11:22:35 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 11:22:35 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 11:27:24 - INFO - __main__ -     eval_loss = 0.2254\n",
      "10/30/2023 11:27:24 - INFO - __main__ -     eval_acc = 0.9186\n",
      "10/30/2023 11:27:24 - INFO - __main__ -     eval_f1 = 0.138\n",
      "epoch 0 loss 0.24871:  90%|████████▉ | 8496/9441 [3:02:14<27:08:49, 103.42s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.24609: 100%|█████████▉| 9439/9441 [3:16:45<00:01,  1.11it/s]    10/30/2023 11:42:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 11:42:45 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 11:42:45 - INFO - __main__ -     Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func OriginalCrossEntropy\\\n",
    "    --BCE_bias 1.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study pos_weight 1, acc best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 12:42:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 12:42:50 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 12:43:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 12:43:49 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 12:43:49 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/30/2023 12:48:26 - INFO - __main__ -   ***** Eval results *****\n",
      "10/30/2023 12:48:26 - INFO - __main__ -     eval_acc = 0.9194\n",
      "10/30/2023 12:48:26 - INFO - __main__ -     eval_f1 = 0.0913\n",
      "10/30/2023 12:48:26 - INFO - __main__ -     eval_loss = 0.2252\n",
      "10/30/2023 12:49:21 - INFO - __main__ -   ***** Running Test *****\n",
      "10/30/2023 12:49:21 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 12:49:21 - INFO - __main__ -     Batch size = 64\n",
      " 75%|███████▍  | 443/591 [03:24<01:07,  2.18it/s]"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.9187299737838626}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study pos_weight 1, f1 based model! (modify the file entry in run.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 13:00:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 13:00:43 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 13:01:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 13:01:42 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 13:01:42 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/30/2023 13:06:19 - INFO - __main__ -   ***** Eval results *****\n",
      "10/30/2023 13:06:19 - INFO - __main__ -     eval_acc = 0.9152\n",
      "10/30/2023 13:06:19 - INFO - __main__ -     eval_f1 = 0.2726\n",
      "10/30/2023 13:06:19 - INFO - __main__ -     eval_loss = 0.2214\n",
      "10/30/2023 13:07:14 - INFO - __main__ -   ***** Running Test *****\n",
      "10/30/2023 13:07:14 - INFO - __main__ -     Num examples = 37763\n",
      "10/30/2023 13:07:14 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 591/591 [04:33<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acc': 0.9159759553001615}\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluator/evaluator.py -a ../dataset/test.jsonl -p ../code/saved_models/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high acc, low f1 -> highly imblanaced dataset !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the improved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 really low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 19:02:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 19:02:35 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   idx: 0\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   label: 1\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   input_tokens: ['<s>', 'int', '__', 'gn', 'utls', '_', 'ciphertext', '2', 'compressed', '(', 'gn', 'utls', '_', 'session', '_', 't', '_session', ',', '_opaque', '_*', '_compress', '_', 'data', ',', '_int', '_compress', '_', 'size', ',', '_gn', 'utls', '_', 'datum', '_', 't', '_ciphertext', ',', '_uint', '8', '_type', ')', '_{', '_uint', '8', '_MAC', '[', 'MAX', '_', 'HASH', '_', 'SIZE', '];', '_uint', '16', '_c', '_', 'length', ';', '_uint', '8', '_pad', ';', '_int', '_length', ';', '_mac', '_', 'hd', '_', 't', '_td', ';', '_uint', '16', '_blocksize', ';', '_int', '_ret', ',', '_i', ',', '_pad', '_', 'failed', '_=', '_0', ';', '_uint', '8', '_major', ',', '_minor', ';', '_gn', 'utls', '_', 'protocol', '_', 't', '_ver', ';', '_int', '_hash', '_', 'size', '_=', '__', 'gn', 'utls', '_', 'hash', '_', 'get', '_', 'algo', '_', 'len', '(', 'session', '->', 'security', '_', 'parameters', '.', '_read', '_', 'mac', '_', 'algorithm', ');', '_ver', '_=', '_gn', 'utls', '_', 'protocol', '_', 'get', '_', 'version', '(', 'session', ');', '_minor', '_=', '__', 'gn', 'utls', '_', 'version', '_', 'get', '_', 'minor', '(', 'ver', ');', '_major', '_=', '__', 'gn', 'utls', '_', 'version', '_', 'get', '_', 'major', '(', 'ver', ');', '_blocksize', '_=', '__', 'gn', 'utls', '_', 'cipher', '_', 'get', '_', 'block', '_', 'size', '(', 'session', '->', 'security', '_', 'parameters', '.', '_read', '_', 'bulk', '_', 'cipher', '_', 'algorithm', ');', '_/*', '_initialize', '_MAC', '_*/', '_td', '_=', '_mac', '_', 'init', '(', 'session', '->', 'security', '_', 'parameters', '.', 'read', '_', 'mac', '_', 'algorithm', ',', '_session', '->', 'connection', '_', 'state', '.', 'read', '_', 'mac', '_', 'secret', '.', 'data', ',', '_session', '->', 'connection', '_', 'state', '.', 'read', '_', 'mac', '_', 'secret', '.', 'size', ',', '_ver', ');', '_if', '_(', 'td', '_==', '_G', 'N', 'UT', 'LS', '_', 'MAC', '_', 'FAILED', '_&&', '_session', '->', 'security', '_', 'parameters', '.', 'read', '_', 'mac', '_', 'algorithm', '_!=', '_G', 'N', 'UT', 'LS', '_', 'MAC', '_', 'NULL', ')', '_{', '_gn', 'utls', '_', 'assert', '();', '_return', '_G', 'N', 'UT', 'LS', '_', 'E', '_', 'INTERNAL', '_', 'ERROR', ';', '_}', '_/*', '_actual', '_decryption', '_(', 'inplace', ')', '_*/', '_switch', '_(_', 'gn', 'utls', '_', 'cipher', '_', 'is', '_', 'block', '_(', 'session', '->', 'security', '_', 'parameters', '.', 'read', '_', 'bulk', '_', 'cipher', '_', 'algorithm', '))', '_{', '_case', '_C', 'IPHER', '_', 'STREAM', ':', '_if', '_((', 'ret', '_=', '__', 'gn', 'utls', '_', 'cipher', '_', 'decrypt', '(', 'session', '->', 'connection', '_', 'state', '.', '_read', '_', 'cipher', '_', 'state', ',', '_ciphertext', '.', 'data', ',', '_ciphertext', '.', 'size', '))', '_<', '_0', ')', '_{', '_gn', 'utls', '_', 'assert', '();', '_return', '_ret', ';', '_}', '_length', '_=', '_ciphertext', '.', 'size', '_-', '_hash', '_', 'size', ';', '_break', '</s>']\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   input_ids: 0 430 623 2728 47787 181 21679 136 9291 126 2728 47787 181 2564 181 202 3157 130 18128 426 14450 181 636 130 554 14450 181 797 130 28542 47787 181 16950 181 202 30164 130 941 142 889 127 399 941 142 8445 177 2485 181 10564 181 2089 823 941 1027 404 181 977 145 941 142 6448 145 554 1977 145 6156 181 3674 181 202 9820 145 941 1027 27079 145 554 1234 130 548 130 6448 181 4844 385 461 145 941 142 14248 130 12438 145 28542 47787 181 5582 181 202 2922 145 554 2712 181 797 385 623 2728 47787 181 2219 181 459 181 15533 181 692 126 2564 408 7587 181 4427 132 1557 181 2978 181 12351 388 2922 385 28542 47787 181 5582 181 459 181 1724 126 2564 388 12438 385 623 2728 47787 181 1724 181 459 181 8641 126 646 388 14248 385 623 2728 47787 181 1724 181 459 181 8736 126 646 388 27079 385 623 2728 47787 181 6874 181 459 181 1501 181 797 126 2564 408 7587 181 4427 132 1557 181 11232 181 6874 181 12351 388 1067 6211 8445 634 9820 385 6156 181 1242 126 2564 408 7587 181 4427 132 814 181 2978 181 12351 130 3157 408 3592 181 1126 132 814 181 2978 181 7547 132 636 130 3157 408 3592 181 1126 132 814 181 2978 181 7547 132 797 130 2922 388 462 400 2619 550 855 164 1286 4056 181 5122 181 6971 698 3157 408 7587 181 4427 132 814 181 2978 181 12351 620 855 164 1286 4056 181 5122 181 2885 127 399 28542 47787 181 1171 523 483 855 164 1286 4056 181 155 181 12488 181 1949 145 425 1067 3780 38856 400 36779 127 634 2444 3389 2728 47787 181 6874 181 402 181 1501 400 2564 408 7587 181 4427 132 814 181 11232 181 6874 181 12351 509 399 883 532 17107 181 8129 144 462 1219 420 385 623 2728 47787 181 6874 181 13751 126 2564 408 3592 181 1126 132 1557 181 6874 181 1126 130 30164 132 636 130 30164 132 797 509 517 461 127 399 28542 47787 181 1171 523 483 1234 145 425 1977 385 30164 132 797 581 2712 181 797 145 1127 2\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   idx: 1\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   label: 0\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   input_tokens: ['<s>', 'void', '_K', 'Password', 'Dlg', '::', 'key', 'Pressed', '(', '_Q', 'KeyEvent', '_*', 'e', '_)', '_{', '_static', '_bool', '_waitFor', 'Authentication', '_=', '_false', ';', '_if', '_(!', 'waitFor', 'Authentication', ')', '_{', '_switch', '_(', '_e', '->', 'key', '()', '_)', '_{', '_case', '_Key', '_', 'Back', 'space', ':', '_{', '_int', '_len', '_=', '_password', '.', 'length', '();', '_if', '_(', '_len', '_)', '_{', '_password', '.', 'truncate', '(', '_len', '_-', '_1', '_);', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_}', '_}', '_break', ';', '_case', '_Key', '_', 'Return', ':', '_timer', '.', 'stop', '();', '_waitFor', 'Authentication', '_=', '_true', ';', '_if', '_(', '_try', 'Password', '()', '_)', '_emit', '_pass', 'Ok', '();', '_else', '_{', '_label', '->', 'setText', '(', '_g', 'locale', '->', 'translate', '(\"', 'Failed', '\")', '_);', '_password', '_=', '_\"\";', '_timer', 'Mode', '_=', '_1', ';', '_timer', '.', 'start', '(', '_1500', ',', '_TRUE', '_);', '_}', '_waitFor', 'Authentication', '_=', '_false', ';', '_break', ';', '_case', '_Key', '_', 'Escape', ':', '_emit', '_pass', 'Cancel', '();', '_break', ';', '_default', ':', '_if', '_(', '_password', '.', 'length', '()', '_<', '_MAX', '_', 'PASSWORD', '_', 'LENGTH', '_)', '_{', '_password', '_+=', '_(', 'char', ')', 'e', '->', 'ascii', '();', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_timer', '.', 'change', 'Interval', '(', '_10000', '_);', '_}', '_}', '_}', '_}', '</s>']\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   input_ids: 0 895 2583 5256 7560 500 671 12528 126 1152 21803 426 187 743 399 1176 1223 20564 9832 385 766 145 462 802 14457 9832 127 399 2444 400 572 408 671 429 743 399 883 4384 181 2432 1428 144 399 554 1015 385 5724 132 977 523 462 400 1015 743 399 5724 132 13327 126 1015 581 524 857 462 126 558 16468 743 3924 10621 201 523 425 425 1127 145 883 4384 181 1675 144 5912 132 3156 523 20564 9832 385 769 145 462 400 1568 5256 429 743 5311 3198 6358 523 669 399 2649 408 8071 126 611 4727 408 7348 503 3624 807 857 5724 385 4986 5912 1649 385 524 145 5912 132 1094 126 35639 130 3876 857 425 20564 9832 385 766 145 1127 145 883 4384 181 9586 144 5311 3198 5686 523 1127 145 1361 144 462 400 5724 132 977 429 517 4789 181 20016 181 6905 743 399 5724 1054 400 1285 127 187 408 9654 523 462 126 558 16468 743 3924 10621 201 523 5912 132 2999 4655 126 12230 857 425 425 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   idx: 2\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   label: 0\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'da', 'dec', '_', 'init', '(', 'AV', 'CodecContext', '_*', 'avctx', ')', '_{', '_V', 'DA', 'Decoder', 'Context', '_*', 'ctx', '_=', '_avctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_ctx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_pix', '_', 'fmts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', ')', '_{', '_if', '_(', 'k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '_<', '_k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', '_', 'prior', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_memset', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_avctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_avctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', \"_'\", 'avc', '1', \"';\", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'decoding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_ctx', '->', 'pix', '_', 'fmt', '_=', '_avctx', '->', 'get', '_', 'format', '(', 'avctx', ',', '_avctx', '->', 'codec', '->', 'pix', '_', 'fmts', ');', '_switch', '_(', 'ctx', '->', 'pix', '_', 'fmt', ')', '_{', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'UY', 'VY', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '2', 'vu', 'y', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YU', 'YV', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'yu', 'vs', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'NV', '12', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '420', 'v', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YUV', '420', 'P', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'y', '420', \"';\", '_break', ';', '_default', ':', '_av', '_', 'log', '(', 'avctx', ',', '_AV', '</s>']\n",
      "10/30/2023 19:09:28 - INFO - __main__ -   input_ids: 0 932 2729 181 37557 554 460 2446 3418 181 1242 126 4543 40294 426 16581 127 399 1010 4910 6706 1133 426 1203 385 24237 408 996 181 636 145 1277 460 2446 181 1499 426 204 2446 181 1203 385 519 1203 408 204 2446 181 1203 145 5027 1536 2104 145 554 1234 145 3025 408 190 12383 181 9617 385 461 145 1067 1796 9718 181 49749 595 10502 634 462 802 544 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 127 399 462 400 193 3196 3699 21876 1877 1934 517 1085 3196 3699 21876 1877 1934 1083 181 141 127 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 181 21982 181 1083 181 141 145 669 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 145 425 1067 1796 460 2446 634 9597 126 204 2446 181 1203 130 461 130 1710 126 627 460 2446 181 1499 648 460 2446 181 1203 408 1729 385 24237 408 1729 145 460 2446 181 1203 408 2047 385 24237 408 2047 145 460 2446 181 1203 408 1478 385 464 48271 135 1177 460 2446 181 1203 408 1031 181 3616 181 43974 385 524 145 460 2446 181 1203 408 1031 181 903 181 1601 385 524 145 3025 408 7009 181 2976 385 24237 408 459 181 1478 126 16581 130 24237 408 4403 408 7009 181 49749 388 2444 400 1203 408 7009 181 2976 127 399 883 7082 181 10526 181 9103 181 22440 20978 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 136 26233 207 1177 1127 145 883 7082 181 10526 181 9103 181 16006 26736 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 36066 4728 1177 1127 145 883 7082 181 10526 181 9103 181 7337 1093 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 21034 204 1177 1127 145 883 7082 181 10526 181 9103 181 19872 21034 166 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 207 21034 1177 1127 145 1361 144 2729 181 896 126 16581 130 7082 2\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/30/2023 19:09:29 - INFO - __main__ -   ***** Running training *****\n",
      "10/30/2023 19:09:29 - INFO - __main__ -     Num examples = 294323\n",
      "10/30/2023 19:09:29 - INFO - __main__ -     Num Epochs = 3\n",
      "10/30/2023 19:09:29 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/30/2023 19:09:29 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/30/2023 19:09:29 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/30/2023 19:09:29 - INFO - __main__ -     Total optimization steps = 27594\n",
      "  0%|          | 0/9198 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.30202:  10%|▉         | 918/9198 [14:05<1:58:52,  1.16it/s]10/30/2023 19:24:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 19:24:25 - INFO - __main__ -     Num examples = 36790\n",
      "10/30/2023 19:24:25 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 19:29:09 - INFO - __main__ -     eval_loss = 0.2575\n",
      "10/30/2023 19:29:09 - INFO - __main__ -     eval_acc = 0.9188\n",
      "10/30/2023 19:29:09 - INFO - __main__ -     eval_f1 = 0.0\n",
      "10/30/2023 19:29:09 - INFO - __main__ -     ********************\n",
      "10/30/2023 19:29:09 - INFO - __main__ -     Best acc:0.9188\n",
      "10/30/2023 19:29:09 - INFO - __main__ -     ********************\n",
      "10/30/2023 19:29:10 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.30202:  10%|▉         | 919/9198 [19:41<233:34:23, 101.57s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.27609:  20%|█▉        | 1837/9198 [33:53<1:45:02,  1.17it/s]  10/30/2023 19:44:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 19:44:14 - INFO - __main__ -     Num examples = 36790\n",
      "10/30/2023 19:44:14 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 19:48:59 - INFO - __main__ -     eval_loss = 0.2416\n",
      "10/30/2023 19:48:59 - INFO - __main__ -     eval_acc = 0.9189\n",
      "10/30/2023 19:48:59 - INFO - __main__ -     eval_f1 = 0.0033\n",
      "10/30/2023 19:48:59 - INFO - __main__ -     ********************\n",
      "10/30/2023 19:48:59 - INFO - __main__ -     Best acc:0.9189\n",
      "10/30/2023 19:48:59 - INFO - __main__ -     ********************\n",
      "10/30/2023 19:49:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 19:49:00 - INFO - __main__ -     ********************\n",
      "10/30/2023 19:49:00 - INFO - __main__ -     Best f1:0.0033\n",
      "10/30/2023 19:49:00 - INFO - __main__ -     ********************\n",
      "10/30/2023 19:49:00 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.27609:  20%|█▉        | 1838/9198 [39:31<209:07:09, 102.29s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.26811:  26%|██▌       | 2386/9198 [47:59<1:58:31,  1.04s/it]   ^C\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func OriginalCrossEntropy\\\n",
    "    --BCE_bias 1.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 19:57:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 19:57:39 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=10.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   idx: 0\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   label: 1\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   input_tokens: ['<s>', 'int', '__', 'gn', 'utls', '_', 'ciphertext', '2', 'compressed', '(', 'gn', 'utls', '_', 'session', '_', 't', '_session', ',', '_opaque', '_*', '_compress', '_', 'data', ',', '_int', '_compress', '_', 'size', ',', '_gn', 'utls', '_', 'datum', '_', 't', '_ciphertext', ',', '_uint', '8', '_type', ')', '_{', '_uint', '8', '_MAC', '[', 'MAX', '_', 'HASH', '_', 'SIZE', '];', '_uint', '16', '_c', '_', 'length', ';', '_uint', '8', '_pad', ';', '_int', '_length', ';', '_mac', '_', 'hd', '_', 't', '_td', ';', '_uint', '16', '_blocksize', ';', '_int', '_ret', ',', '_i', ',', '_pad', '_', 'failed', '_=', '_0', ';', '_uint', '8', '_major', ',', '_minor', ';', '_gn', 'utls', '_', 'protocol', '_', 't', '_ver', ';', '_int', '_hash', '_', 'size', '_=', '__', 'gn', 'utls', '_', 'hash', '_', 'get', '_', 'algo', '_', 'len', '(', 'session', '->', 'security', '_', 'parameters', '.', '_read', '_', 'mac', '_', 'algorithm', ');', '_ver', '_=', '_gn', 'utls', '_', 'protocol', '_', 'get', '_', 'version', '(', 'session', ');', '_minor', '_=', '__', 'gn', 'utls', '_', 'version', '_', 'get', '_', 'minor', '(', 'ver', ');', '_major', '_=', '__', 'gn', 'utls', '_', 'version', '_', 'get', '_', 'major', '(', 'ver', ');', '_blocksize', '_=', '__', 'gn', 'utls', '_', 'cipher', '_', 'get', '_', 'block', '_', 'size', '(', 'session', '->', 'security', '_', 'parameters', '.', '_read', '_', 'bulk', '_', 'cipher', '_', 'algorithm', ');', '_/*', '_initialize', '_MAC', '_*/', '_td', '_=', '_mac', '_', 'init', '(', 'session', '->', 'security', '_', 'parameters', '.', 'read', '_', 'mac', '_', 'algorithm', ',', '_session', '->', 'connection', '_', 'state', '.', 'read', '_', 'mac', '_', 'secret', '.', 'data', ',', '_session', '->', 'connection', '_', 'state', '.', 'read', '_', 'mac', '_', 'secret', '.', 'size', ',', '_ver', ');', '_if', '_(', 'td', '_==', '_G', 'N', 'UT', 'LS', '_', 'MAC', '_', 'FAILED', '_&&', '_session', '->', 'security', '_', 'parameters', '.', 'read', '_', 'mac', '_', 'algorithm', '_!=', '_G', 'N', 'UT', 'LS', '_', 'MAC', '_', 'NULL', ')', '_{', '_gn', 'utls', '_', 'assert', '();', '_return', '_G', 'N', 'UT', 'LS', '_', 'E', '_', 'INTERNAL', '_', 'ERROR', ';', '_}', '_/*', '_actual', '_decryption', '_(', 'inplace', ')', '_*/', '_switch', '_(_', 'gn', 'utls', '_', 'cipher', '_', 'is', '_', 'block', '_(', 'session', '->', 'security', '_', 'parameters', '.', 'read', '_', 'bulk', '_', 'cipher', '_', 'algorithm', '))', '_{', '_case', '_C', 'IPHER', '_', 'STREAM', ':', '_if', '_((', 'ret', '_=', '__', 'gn', 'utls', '_', 'cipher', '_', 'decrypt', '(', 'session', '->', 'connection', '_', 'state', '.', '_read', '_', 'cipher', '_', 'state', ',', '_ciphertext', '.', 'data', ',', '_ciphertext', '.', 'size', '))', '_<', '_0', ')', '_{', '_gn', 'utls', '_', 'assert', '();', '_return', '_ret', ';', '_}', '_length', '_=', '_ciphertext', '.', 'size', '_-', '_hash', '_', 'size', ';', '_break', '</s>']\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   input_ids: 0 430 623 2728 47787 181 21679 136 9291 126 2728 47787 181 2564 181 202 3157 130 18128 426 14450 181 636 130 554 14450 181 797 130 28542 47787 181 16950 181 202 30164 130 941 142 889 127 399 941 142 8445 177 2485 181 10564 181 2089 823 941 1027 404 181 977 145 941 142 6448 145 554 1977 145 6156 181 3674 181 202 9820 145 941 1027 27079 145 554 1234 130 548 130 6448 181 4844 385 461 145 941 142 14248 130 12438 145 28542 47787 181 5582 181 202 2922 145 554 2712 181 797 385 623 2728 47787 181 2219 181 459 181 15533 181 692 126 2564 408 7587 181 4427 132 1557 181 2978 181 12351 388 2922 385 28542 47787 181 5582 181 459 181 1724 126 2564 388 12438 385 623 2728 47787 181 1724 181 459 181 8641 126 646 388 14248 385 623 2728 47787 181 1724 181 459 181 8736 126 646 388 27079 385 623 2728 47787 181 6874 181 459 181 1501 181 797 126 2564 408 7587 181 4427 132 1557 181 11232 181 6874 181 12351 388 1067 6211 8445 634 9820 385 6156 181 1242 126 2564 408 7587 181 4427 132 814 181 2978 181 12351 130 3157 408 3592 181 1126 132 814 181 2978 181 7547 132 636 130 3157 408 3592 181 1126 132 814 181 2978 181 7547 132 797 130 2922 388 462 400 2619 550 855 164 1286 4056 181 5122 181 6971 698 3157 408 7587 181 4427 132 814 181 2978 181 12351 620 855 164 1286 4056 181 5122 181 2885 127 399 28542 47787 181 1171 523 483 855 164 1286 4056 181 155 181 12488 181 1949 145 425 1067 3780 38856 400 36779 127 634 2444 3389 2728 47787 181 6874 181 402 181 1501 400 2564 408 7587 181 4427 132 814 181 11232 181 6874 181 12351 509 399 883 532 17107 181 8129 144 462 1219 420 385 623 2728 47787 181 6874 181 13751 126 2564 408 3592 181 1126 132 1557 181 6874 181 1126 130 30164 132 636 130 30164 132 797 509 517 461 127 399 28542 47787 181 1171 523 483 1234 145 425 1977 385 30164 132 797 581 2712 181 797 145 1127 2\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   idx: 1\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   label: 0\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   input_tokens: ['<s>', 'void', '_K', 'Password', 'Dlg', '::', 'key', 'Pressed', '(', '_Q', 'KeyEvent', '_*', 'e', '_)', '_{', '_static', '_bool', '_waitFor', 'Authentication', '_=', '_false', ';', '_if', '_(!', 'waitFor', 'Authentication', ')', '_{', '_switch', '_(', '_e', '->', 'key', '()', '_)', '_{', '_case', '_Key', '_', 'Back', 'space', ':', '_{', '_int', '_len', '_=', '_password', '.', 'length', '();', '_if', '_(', '_len', '_)', '_{', '_password', '.', 'truncate', '(', '_len', '_-', '_1', '_);', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_}', '_}', '_break', ';', '_case', '_Key', '_', 'Return', ':', '_timer', '.', 'stop', '();', '_waitFor', 'Authentication', '_=', '_true', ';', '_if', '_(', '_try', 'Password', '()', '_)', '_emit', '_pass', 'Ok', '();', '_else', '_{', '_label', '->', 'setText', '(', '_g', 'locale', '->', 'translate', '(\"', 'Failed', '\")', '_);', '_password', '_=', '_\"\";', '_timer', 'Mode', '_=', '_1', ';', '_timer', '.', 'start', '(', '_1500', ',', '_TRUE', '_);', '_}', '_waitFor', 'Authentication', '_=', '_false', ';', '_break', ';', '_case', '_Key', '_', 'Escape', ':', '_emit', '_pass', 'Cancel', '();', '_break', ';', '_default', ':', '_if', '_(', '_password', '.', 'length', '()', '_<', '_MAX', '_', 'PASSWORD', '_', 'LENGTH', '_)', '_{', '_password', '_+=', '_(', 'char', ')', 'e', '->', 'ascii', '();', '_if', '(', '_st', 'ars', '_)', '_show', 'Star', 's', '();', '_timer', '.', 'change', 'Interval', '(', '_10000', '_);', '_}', '_}', '_}', '_}', '</s>']\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   input_ids: 0 895 2583 5256 7560 500 671 12528 126 1152 21803 426 187 743 399 1176 1223 20564 9832 385 766 145 462 802 14457 9832 127 399 2444 400 572 408 671 429 743 399 883 4384 181 2432 1428 144 399 554 1015 385 5724 132 977 523 462 400 1015 743 399 5724 132 13327 126 1015 581 524 857 462 126 558 16468 743 3924 10621 201 523 425 425 1127 145 883 4384 181 1675 144 5912 132 3156 523 20564 9832 385 769 145 462 400 1568 5256 429 743 5311 3198 6358 523 669 399 2649 408 8071 126 611 4727 408 7348 503 3624 807 857 5724 385 4986 5912 1649 385 524 145 5912 132 1094 126 35639 130 3876 857 425 20564 9832 385 766 145 1127 145 883 4384 181 9586 144 5311 3198 5686 523 1127 145 1361 144 462 400 5724 132 977 429 517 4789 181 20016 181 6905 743 399 5724 1054 400 1285 127 187 408 9654 523 462 126 558 16468 743 3924 10621 201 523 5912 132 2999 4655 126 12230 857 425 425 425 425 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   *** Example ***\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   idx: 2\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   label: 0\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'da', 'dec', '_', 'init', '(', 'AV', 'CodecContext', '_*', 'avctx', ')', '_{', '_V', 'DA', 'Decoder', 'Context', '_*', 'ctx', '_=', '_avctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_ctx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_pix', '_', 'fmts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', ')', '_{', '_if', '_(', 'k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '_<', '_k', 'CF', 'Core', 'Foundation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', '_', 'prior', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'decoder', '.', 'pix', '_', 'fmts', '_=', '_v', 'da', '_', 'pix', 'fmts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_memset', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_avctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_avctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', \"_'\", 'avc', '1', \"';\", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'decoding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_ctx', '->', 'pix', '_', 'fmt', '_=', '_avctx', '->', 'get', '_', 'format', '(', 'avctx', ',', '_avctx', '->', 'codec', '->', 'pix', '_', 'fmts', ');', '_switch', '_(', 'ctx', '->', 'pix', '_', 'fmt', ')', '_{', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'UY', 'VY', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '2', 'vu', 'y', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YU', 'YV', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'yu', 'vs', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'NV', '12', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", '420', 'v', \"';\", '_break', ';', '_case', '_AV', '_', 'PIX', '_', 'FMT', '_', 'YUV', '420', 'P', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'pix', '_', 'fmt', '_', 'type', '_=', \"_'\", 'y', '420', \"';\", '_break', ';', '_default', ':', '_av', '_', 'log', '(', 'avctx', ',', '_AV', '</s>']\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   input_ids: 0 932 2729 181 37557 554 460 2446 3418 181 1242 126 4543 40294 426 16581 127 399 1010 4910 6706 1133 426 1203 385 24237 408 996 181 636 145 1277 460 2446 181 1499 426 204 2446 181 1203 385 519 1203 408 204 2446 181 1203 145 5027 1536 2104 145 554 1234 145 3025 408 190 12383 181 9617 385 461 145 1067 1796 9718 181 49749 595 10502 634 462 802 544 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 127 399 462 400 193 3196 3699 21876 1877 1934 517 1085 3196 3699 21876 1877 1934 1083 181 141 127 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 181 21982 181 1083 181 141 145 669 11384 181 190 12383 181 204 2446 181 8464 132 7009 181 49749 385 460 2446 181 7009 49749 145 425 1067 1796 460 2446 634 9597 126 204 2446 181 1203 130 461 130 1710 126 627 460 2446 181 1499 648 460 2446 181 1203 408 1729 385 24237 408 1729 145 460 2446 181 1203 408 2047 385 24237 408 2047 145 460 2446 181 1203 408 1478 385 464 48271 135 1177 460 2446 181 1203 408 1031 181 3616 181 43974 385 524 145 460 2446 181 1203 408 1031 181 903 181 1601 385 524 145 3025 408 7009 181 2976 385 24237 408 459 181 1478 126 16581 130 24237 408 4403 408 7009 181 49749 388 2444 400 1203 408 7009 181 2976 127 399 883 7082 181 10526 181 9103 181 22440 20978 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 136 26233 207 1177 1127 145 883 7082 181 10526 181 9103 181 16006 26736 24181 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 36066 4728 1177 1127 145 883 7082 181 10526 181 9103 181 7337 1093 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 21034 204 1177 1127 145 883 7082 181 10526 181 9103 181 19872 21034 166 144 460 2446 181 1203 408 4528 181 7009 181 2976 181 683 385 464 207 21034 1177 1127 145 1361 144 2729 181 896 126 16581 130 7082 2\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "10/30/2023 20:04:33 - INFO - __main__ -   ***** Running training *****\n",
      "10/30/2023 20:04:33 - INFO - __main__ -     Num examples = 294323\n",
      "10/30/2023 20:04:33 - INFO - __main__ -     Num Epochs = 3\n",
      "10/30/2023 20:04:33 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "10/30/2023 20:04:33 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "10/30/2023 20:04:33 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "10/30/2023 20:04:33 - INFO - __main__ -     Total optimization steps = 27594\n",
      "  0%|          | 0/9198 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.30202:  10%|▉         | 918/9198 [14:18<1:59:45,  1.15it/s]10/30/2023 20:19:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 20:19:42 - INFO - __main__ -     Num examples = 36790\n",
      "10/30/2023 20:19:42 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 20:24:28 - INFO - __main__ -     eval_loss = 0.2575\n",
      "10/30/2023 20:24:28 - INFO - __main__ -     eval_acc = 0.9188\n",
      "10/30/2023 20:24:28 - INFO - __main__ -     eval_f1 = 0.0\n",
      "10/30/2023 20:24:28 - INFO - __main__ -     ********************\n",
      "10/30/2023 20:24:28 - INFO - __main__ -     Best acc:0.9188\n",
      "10/30/2023 20:24:28 - INFO - __main__ -     ********************\n",
      "10/30/2023 20:24:29 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "epoch 0 loss 0.30202:  10%|▉         | 919/9198 [19:55<234:35:29, 102.01s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.27609:  20%|█▉        | 1837/9198 [33:58<2:11:48,  1.07s/it]  10/30/2023 20:39:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 20:39:21 - INFO - __main__ -     Num examples = 36790\n",
      "10/30/2023 20:39:21 - INFO - __main__ -     Batch size = 64\n",
      "10/30/2023 20:44:04 - INFO - __main__ -     eval_loss = 0.2416\n",
      "10/30/2023 20:44:04 - INFO - __main__ -     eval_acc = 0.9189\n",
      "10/30/2023 20:44:04 - INFO - __main__ -     eval_f1 = 0.0033\n",
      "10/30/2023 20:44:04 - INFO - __main__ -     ********************\n",
      "10/30/2023 20:44:04 - INFO - __main__ -     Best acc:0.9189\n",
      "10/30/2023 20:44:04 - INFO - __main__ -     ********************\n",
      "10/30/2023 20:44:05 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-acc/model.bin\n",
      "10/30/2023 20:44:05 - INFO - __main__ -     ********************\n",
      "10/30/2023 20:44:05 - INFO - __main__ -     Best f1:0.0033\n",
      "10/30/2023 20:44:05 - INFO - __main__ -     ********************\n",
      "10/30/2023 20:44:06 - INFO - __main__ -   Saving model checkpoint to ./saved_models/checkpoint-best-f1/model.bin\n",
      "epoch 0 loss 0.27609:  20%|█▉        | 1838/9198 [39:32<206:41:53, 101.10s/it]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 0 loss 0.27439:  21%|██        | 1952/9198 [41:17<1:44:06,  1.16it/s]   ^C\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_train \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --loss_func BCEWithLogits\\\n",
    "    --BCE_bias 10.0\\\n",
    "    --seed 123456  2>&1 | tee train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test large dataset trained model tested on devign\n",
    "#Large model is undertrained. Accuracy and f1 decreased.\n",
    "#NOTE: Test set of devign is NOT included in the train set of composed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2023 23:10:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 8, distributed training: False, 16-bits training: False\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base-nine and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/30/2023 23:10:58 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/unixcoder-base-nine', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/unixcoder-base-nine', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, loss_func='OriginalCrossEntropy', BCE_bias=1.0, n_gpu=8, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, start_epoch=0, start_step=0)\n",
      "10/30/2023 23:11:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "10/30/2023 23:11:04 - INFO - __main__ -     Num examples = 2732\n",
      "10/30/2023 23:11:04 - INFO - __main__ -     Batch size = 64\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "10/30/2023 23:11:21 - INFO - __main__ -   ***** Eval results *****\n",
      "10/30/2023 23:11:21 - INFO - __main__ -     eval_acc = 0.6157\n",
      "10/30/2023 23:11:21 - INFO - __main__ -     eval_f1 = 0.5607\n",
      "10/30/2023 23:11:21 - INFO - __main__ -     eval_loss = 0.7966\n",
      "10/30/2023 23:11:27 - INFO - __main__ -   ***** Running Test *****\n",
      "10/30/2023 23:11:27 - INFO - __main__ -     Num examples = 2732\n",
      "10/30/2023 23:11:27 - INFO - __main__ -     Batch size = 64\n",
      "100%|██████████| 43/43 [00:10<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --output_dir=./saved_models \\\n",
    "    --model_type=roberta \\\n",
    "    --tokenizer_name=microsoft/unixcoder-base-nine \\\n",
    "    --model_name_or_path=microsoft/unixcoder-base-nine \\\n",
    "    --do_eval \\\n",
    "    --do_test \\\n",
    "    --train_data_file=../dataset/train.jsonl \\\n",
    "    --eval_data_file=../dataset/valid.jsonl \\\n",
    "    --test_data_file=../dataset/test.jsonl \\\n",
    "    --epoch 3 \\\n",
    "    --block_size 400 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --evaluate_during_training \\\n",
    "    --seed 123456 2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
